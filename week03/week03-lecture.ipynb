{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8174e519",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **Fall 2025 &mdash; CIS 3803<br>Introduction to Data Science**\n",
    "### Week 3: Pandas Tutorial\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5f0a6",
   "metadata": {},
   "source": [
    "**Date:** 15 September 2025  \n",
    "**Time:** 6:00–9:00 PM  \n",
    "**Instructor:** Dr. Patrick T. Marsh  \n",
    "**Course Verse:** “It is the glory of God to conceal a matter; to search out a matter is the glory of kings.” &mdash; *Proverbs 25:2 (NIV)*\n",
    "\n",
    "This notebook provides a crash course in Pandas and begins to cover the skills needed to be successful in data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd2a51",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "## **Opening Devotional and Reflection**\n",
    "\n",
    "*\"The Spirit you received does not make you slaves, so that you live in fear again; rather, the Spirit you received brought about your adoption to sonship. And by him we cry, 'Abba, Father.'\"*\n",
    "\n",
    "**&mdash; Romans 8:15 (NIV)**\n",
    "\n",
    "#### **Faith Reflection:** \n",
    "As we continue our work with data, we often face a spirit of fear—fear of messy data, fear of making mistakes, or the fear that our skills aren't good enough. This can make us feel like slaves to the process. But today's verse reminds us that we are adopted as God’s children. He has not given us a spirit of fear, but a spirit of love and a sound mind. As we face the tedious and often frustrating task of data cleaning and preparation, we can trust that we have the wisdom and patience to bring order out of chaos. How can you approach this week's data wrangling challenge with a spirit of confidence, knowing that God has equipped you for the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62909d0e",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "## **Pandas**\n",
    "\n",
    "Pandas is an open-source software library for Python, widely used for data analysis and manipulation. Pandas has two powerful, and easy-to-use data structures:\n",
    "\n",
    "- **Series:** A one-dimensional labeled array capable of holding any data type. Think of it as a single column in a spreadsheet.\n",
    "- **DataFrame:** A two-dimensional labeled data structure with columns of potentially different types. It's like a whole spreadsheet or SQL table.\n",
    "\n",
    "The common way of importing Pandas into a Python script is:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "This is the import convention I will use throughout the semester.\n",
    "\n",
    "#### **Series**\n",
    "\n",
    "Pandas Series are:\n",
    "\n",
    "- **One dimensional:** It has only one axis.\n",
    "- **Labeled:** Each element in a Series has an associated *index*. This index can be a sequence of integers (default), or it can be custom labels (like dates, strings, etc.). This labeling allows for flexible data aceess and alignment.\n",
    "- **Homogeneous Data Type:** While Pandas is flexible, a single Series generally holds elements of the same data type for optimal performance.\n",
    "\n",
    "You can create a Series in several ways:\n",
    "\n",
    "- From a list or array &mdash; and the default integer index is automatically generated\n",
    "- From a list or array with a custom index\n",
    "- From a dictionary &mdash; and the dictionary keys become the Series index\n",
    "\n",
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cccb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a Pandas Series from a list\n",
    "data = [10, 20, 30, 40, 50]\n",
    "s = pd.Series(data)\n",
    "print(f\"Pandas Series from list:\")\n",
    "print(s)\n",
    "\n",
    "# Create a Pandas Series with custom index\n",
    "s_custom_index = pd.Series(data, index=['a', 'b', 'c', 'd', 'e'])\n",
    "print(f\"\\nPandas Series with custom index:\")\n",
    "print(s_custom_index)\n",
    "\n",
    "# Create a Pandas Series from a dictionary\n",
    "data_dict = {'apple': 1, 'banana': 2, 'cherry': 3}\n",
    "s_dict = pd.Series(data_dict)\n",
    "print(f\"\\nPandas Series from dictionary:\")\n",
    "print(s_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e7f042",
   "metadata": {},
   "source": [
    "#### **DataFrame**\n",
    "\n",
    "Pandas DataFrames are the most commonly used Pandas object. They are:\n",
    "\n",
    "- **Two-dimensional:** It has both rows and columns\n",
    "- **Labeled Axes:** Both rows and columns have an index. Rows have row labels (index) and columns have column labels (headers).\n",
    "- **Heterogeneous Data Types:** Columns can contain different data types (e.g., one column can be integers, another strings, another booleans).\n",
    "- **Size Mutable:** You can add or delete rows and columns\n",
    "\n",
    "You can create a DataFrame in multiple ways:\n",
    "\n",
    "- From a dictionary of lists or arrays\n",
    "- From a list of dictionaries\n",
    "\n",
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dictionary of lists or arrays\n",
    "data_df = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 22, 35],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n",
    "}\n",
    "df = pd.DataFrame(data_df)\n",
    "print(f\"\\nPandas Series from dictionary:\")\n",
    "print(df)\n",
    "\n",
    "# List of Dictionaries\n",
    "data_list_dict = [\n",
    "    {'Name': 'Alice', 'Age': 25, 'City': 'New York'},\n",
    "    {'Name': 'Bob', 'Age': 30, 'City': 'Los Angeles'},\n",
    "    {'Name': 'Charlie', 'Age': 22, 'City': 'Chicago'},\n",
    "    {'Name': 'David', 'Age': 35, 'City': 'Houston'}\n",
    "]\n",
    "df_list = pd.DataFrame(data_list_dict)\n",
    "print(f\"\\nDataFrame from list of dictionaries:\")\n",
    "print(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408e4f6",
   "metadata": {},
   "source": [
    "### **Reading and Writing Data**\n",
    "\n",
    "Reading and writing files is a fundamental part of data science. Pandas makes it easy to handle various file formats. Here, we'll focus on CSV, Excel, and JSON.\n",
    "\n",
    "#### Reading Data\n",
    "\n",
    "Pandas provides a suite of `read_****()` functions, which are listed below, that can be used to read data into Pandas. The ones in bold are the functions we are most likely to encounter this semester.\n",
    "\n",
    "- Reading from Text and Web\n",
    "    - `pd.read_clipboard()`: Reads the contents of the clipboard into a DataFrame. This is useful for quickly getting data from a table copied from a website or a spreadsheet.\n",
    "    - **`pd.read_csv()`: The most common function. It reads any delimited text file into a DataFrame (default is comma). It can also handle headers and different encoding formats.**\n",
    "    - `pd.read_fwf()`: Reads a fixed-width formatted file. This is for data where coluns are aligned by character position rather than by a delimter. \n",
    "    - `pd.read_html()`: Reads HTML tables from a URL, a string, or local file path and returns a list of DataFrames\n",
    "    - `pd.read_json()`: Reads a JSON (JavaScript Object Notation) file or JSON string into a DataFrame. It can handle various JSON formats, specified by the `orient` paramter.\n",
    "    - `pd.read_table()`: Similar to `pd.read_csv()`, but the default delimiter is a tab (`\\t`). It's commonly used for reading tab-separated value (TSV) files. \n",
    "    - `pd.read_xml()`: Reads an XML file into a DataFrame. It can parse data from XML tags and attributes.  \n",
    "- Reading from Databases\n",
    "    - `pd.read_gbq()`: Reads data from Google BigQuery into a DataFrame. This function requires the `pandas-gbq` library and proper authentication.\n",
    "    - **`pd.read_sql()`: A general-purpose function for reading an SQL query or a database table into a DataFrame. It requires a database connection string.**\n",
    "    - `pd.read_sql_query()`: Reads the results of an SQL query from a database directly into a DataFrame.\n",
    "    - `pd.read_sql_table()`: Reads an entire table from a database into a DataFrame.\n",
    "- Reading from Binary and Compressed Files\n",
    "    - **`pd.read_excel()`: Reads data from an Excel file (`.xls`, `.xlsx`). It can read from a specific sheet by name or index into a DataFrame. It requires the `openpyxl` library to be installed (via conda).**\n",
    "    - `pd.read_feather()`: Reads as Feather file, a fast, lightweight, and language-agnostic format for storing data frames.\n",
    "    - `pd.read_hdf()`: Reads an HDF5 (Hierarchical Data Format) file, which is designed for storing large amounts of data.\n",
    "    - `pd.read_orc()`: Reads an ORC (Optimized Row Columnar) file, a free and open-source file format for columnar storage.\n",
    "    - `pd.read_parquet()`: Reads a Parquet file, a columnar storage format that's highly efficient for large-scale data processing.\n",
    "    - `pd.read_pickle()`: Reads a pickled (serialized) Python object from a file. This is a very fast way to save and load Pandas objects. \n",
    "- Reading from Statistical Software\n",
    "    - `pd.read_sas()`: Reads a SAS (Statistical Analysis System) file. It can handle both `.sas7bdat` and XPORT files.\n",
    "    - `pd.read_spss()`: Reads an SPSS (Statistical Package for the Social Sciences) file.\n",
    "    - `pd.read_stata()`: Reads a Stata file. \n",
    "\n",
    "#### Writing Data\n",
    "\n",
    "Pandas also provides a suite of `to_****()` functions, which are listed below, that can be used to convert or expert you DataFrame. The ones in bold are the functions we are most likely to encounter this semester. **Note: Not all of these methods create an output file! Some simply return the object in a new format in memory!**\n",
    "\n",
    "- Text and Web Formats\n",
    "    - `to_clipboard()`: Writes the object to the user's clipboard. This is useful for quickly pasting data into other applications like spreadsheets.\n",
    "    - **`to_csv()`: Writes the DataFrame to a comma-separated values (CSV) file. You can specify the delimiter, whether to include the index, and other file properties.**\n",
    "    - `to_dict()`: Converts the DataFrame to a Python dictionary. This is great for integrating with other Python code or for creating structured data objects.\n",
    "    - `to_html()`: Renders the DataFrame as an HTML table string. This is useful for displaying data in web reports.\n",
    "    - `to_json()`: Converts the DataFrame to a JSON (JavaScript Object Notation) string or file. The `orient` parameter controls the JSON format, such as `records` or `split`.\n",
    "    - `to_latex()`: Renders the DataFrame as a LaTeX table. This is perfect for including tables in academic papers or documents written in LaTeX.\n",
    "    - `to_markdown()`: Converts the DataFrame to a Markdown table string, ideal for documentation or README files.\n",
    "    - `to_string()`: Renders the DataFrame as a string representation, suitable for printing to the console or writing to a text file.\n",
    "    - `to_xml()`: Writes the DataFrame to an XML file. This method provides options to define the XML structure and tags.\n",
    "- Binary and Database Formats\n",
    "    - **`to_excel()`: Writes the DataFrame to an Excel file. You can write to a specific sheet and use the Pandas built-it `ExcelWriter` for multiple `DataFrames` in one file. (Note: You still must have the `openpyxl` library installed to write Excel files!**\n",
    "    - `to_feather()`: Writes the DataFrame to the Apache Feather format, a fast, language-agnostic binary format for storing data frames, which is highly efficient for data transfer between Python and other languages like R.\n",
    "    - `to_hdf()`: Writes the DataFrame to an HDF5 file. This is ideal for storing very large, hierarchical datasets.\n",
    "    - `to_numpy()`: Converts the DataFrame to a NumPy `ndarray`. This is a crucial step for many machine learning tasks that require NumPy arrays as input. \n",
    "    - `to_orc()`: Writes the DataFrame to an ORC file, a columnar storage format optimized for large-scale data processing in big data ecosystems.\n",
    "    - `to_parquet()`: Writes the DataFrame to a Parquet file, another highly efficient columnar storage format widely used in distributed computing frameworks like Apache Spark.\n",
    "    - `to_pickle()`: Serializes the DataFrame to a Python pickle file. This is the most efficient way to save and load a pandas object, as it preserves all data types and metadata.\n",
    "    - `to_sql()`: Writes the records in the DataFrame to a SQL database table. This requires a database connection and table name.\n",
    "- Specialty Formats and Transformations\n",
    "    - `to_gbq()`: Writes the DataFrame to a Google BigQuery table. This is part of the `pandas-gbq` library and is useful for cloud-based data warehousing.\n",
    "    - `to_records()`: Converts the DataFrame to a NumPy `recarray` (record array), where each row is a record that can be accessed by field name.\n",
    "    - `to_stata()`: Writes the DataFrame to a Stata file (`.dta`), a format used by the statistical software Stata.\n",
    "    - `to_xarray()`: Converts the DataFrame to an `xarray` object, which is useful for working with multi-dimensional labeled arrays, often used in geosciences and climatology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907eff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "cwd = pathlib.Path().resolve()\n",
    "datadir = cwd.joinpath('example-files')\n",
    "\n",
    "# Read a CSV file\n",
    "df_csv = pd.read_csv(datadir.joinpath('data.csv'), header=None)\n",
    "print(f\"\\nDataFrame from CSV file:\")\n",
    "print(df_csv)\n",
    "\n",
    "# Read a CSV with a different delimiter (e.g., a semicolon)\n",
    "df_semicolon = pd.read_csv(datadir.joinpath('data-semicolon.csv'), sep=';')\n",
    "print(f\"\\nDataFrame from CSV file (semicolon delimited):\")\n",
    "print(df_semicolon)\n",
    "\n",
    "# Read a CSV and specify a column as the index\n",
    "df_indexed = pd.read_csv(datadir.joinpath('data.csv'), index_col='id')\n",
    "print(f\"\\nDataFrame from CSV file (indexed):\")\n",
    "print(df_indexed)\n",
    "\n",
    "# Read a CSV and skip the first few rows\n",
    "df_skipped = pd.read_csv(datadir.joinpath('data.csv'), skiprows=5, header=None)\n",
    "print(f\"\\nDataFrame from CSV file (skipped rows):\")\n",
    "print(df_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f28a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first sheet of an Excel file\n",
    "df_excel = pd.read_excel(datadir.joinpath('data.xlsx'))\n",
    "print(f\"DataFrame from Excel file:\")\n",
    "print(df_excel)\n",
    "\n",
    "# Read a specific sheet by name\n",
    "df_sheet2 = pd.read_excel(datadir.joinpath('data.xlsx'), sheet_name='Sheet2')\n",
    "print(f\"\\nDataFrame from Excel file (Sheet2):\")\n",
    "print(df_sheet2)\n",
    "\n",
    "# Read a specific sheet by index (0-based)\n",
    "df_sheet1 = pd.read_excel(datadir.joinpath('data.xlsx'), sheet_name=0)\n",
    "print(f\"\\nDataFrame from Excel file (Sheet1):\")\n",
    "print(df_sheet1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a JSON file\n",
    "df_json = pd.read_json(datadir.joinpath('data.json'))\n",
    "print(f\"DataFrame from JSON file:\")\n",
    "print(df_json)\n",
    "\n",
    "# Reading a JSON file with a different orientation\n",
    "# This is useful when the JSON is formatted as a list of records\n",
    "df_json_records = pd.read_json(datadir.joinpath('data-records.json'), orient='records')\n",
    "print(f\"\\nDataFrame from JSON file (records):\")\n",
    "print(df_json_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2441d61",
   "metadata": {},
   "source": [
    "### Inspecting Data\n",
    "\n",
    "Once you've loaded data into a Pandas DataFrame, the first step is to get a general overview of the dataset. \n",
    "\n",
    "- **`.head()` and `.tail()`**: These are the most common functions for quickly viewing the top or bottom rows of your data. By default, they show the first or last 5 rows, but you can specify a different number.\n",
    "- **`.info()`**: This provides a concise summary of the DataFrame. It shows the number of entries, column names, the number of non-null values in each column, and the data types (`Dtype`). This is essential for identifying missing data and incorrect data types.\n",
    "- **`.shape`**: This attribute returns a tuple representing the dimensions of the DataFrame, in the format `(rows, columns)`. It's a quick way to see how much data you're working with.\n",
    " - **`.columns`**: This returns a list of the column names in the DataFrame.\n",
    "\n",
    "#### Statistical Summary\n",
    "\n",
    "To understand the distribution of your data, use these functions.\n",
    "\n",
    "- **`.describe()`**: This is a powerful function that generates descriptive statistics for all numerical columns. It includes `count`, `mean`, `std` (standard deviation), `min`, `max`, and the quartile values. For non-numerical data (like strings or objects), you can add the `include='all'` parameter to see the count, unique values, and frequency of the most common values.\n",
    "- **`.value_counts()`**: This is perfect for categorical data. It returns a `Series` containing the counts of unique values in a specified column, sorted in descending order.\n",
    "- **`.unique()`**: This function returns an array of the unique values in a column, without their counts.\n",
    "\n",
    "#### Handling Missing Values (more later)\n",
    "\n",
    "Dealing with missing data is a critical part of data inspection.\n",
    "\n",
    "- **`.isnull()` / `.isna()`**: These methods return a DataFrame of booleans, where `True` indicates a missing value (`NaN`). You can combine them with `sum()` to get a count of missing values per column.\n",
    "- **`.notnull()` / `.notna()`**: The inverse of the above, returning `True` for non-missing values.\n",
    "\n",
    "A typical workflow for initial data inspection is to use a combination of these methods:\n",
    "\n",
    "1.  **`df.info()`** to check data types and find columns with a different number of non-null values.\n",
    "2.  **`df.isnull().sum()`** to get a precise count of missing values per column.\n",
    "3.  **`df.describe()`** to understand the distribution of numerical data.\n",
    "4.  **`df.value_counts()`** on categorical columns to check their frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 rows\n",
    "print(\"First 5 rows of the DataFrame using .head():\")\n",
    "print(df_csv.head())\n",
    "\n",
    "# View the last 7 rows\n",
    "print(\"\\nLast 7 rows of the DataFrame using .tail(7):\")\n",
    "print(df_csv.tail(7))\n",
    "\n",
    "# View the summary of the DataFrame\n",
    "print(\"\\nSummary of the DataFrame:\")\n",
    "df_csv.info()\n",
    "\n",
    "# Get the number of rows and columns\n",
    "print(\"\\nNumber of rows and columns:\")\n",
    "print(df_csv.shape)\n",
    "\n",
    "# Get the column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(df_csv.columns)\n",
    "\n",
    "# Get descriptive statistics for numerical columns\n",
    "print(\"\\nDescriptive statistics for numerical columns:\")\n",
    "print(df_csv.describe())\n",
    "\n",
    "# Get descriptive statistics for all columns\n",
    "print(\"\\nDescriptive statistics for all columns:\")\n",
    "print(df_csv.describe(include='all'))\n",
    "\n",
    "# Get the frequency of each unique value in the 'City' column\n",
    "print(\"\\nFrequency of each unique value in the 'city' column:\")\n",
    "print(df_csv['city'].value_counts())\n",
    "\n",
    "# Find all unique cities\n",
    "print(\"\\nUnique cities:\")\n",
    "print(df_csv['city'].unique())\n",
    "\n",
    "# Check for missing values in the DataFrame\n",
    "print(\"\\nMissing values in the DataFrame:\")\n",
    "print(df_csv.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782fa44",
   "metadata": {},
   "source": [
    "### Data Selection and Indexing\n",
    "\n",
    "As a budding data scientist you need to understand how to access and manipulate subsets of your data. This is a fundamental skill for any data science task, from cleaning to analysis. Pandas offers several methods for this, primarily `[]`, `.loc[]`, and `.iloc[]`. Within these methods, you can select data, index data, and slice data.\n",
    "\n",
    "#### Basic Selection with `[]`\n",
    "\n",
    "The most straightforward method for selecting data is using the `[]` operator.\n",
    "\n",
    "- **Selecting a single column:** Use the column name inside the brackets. This returns a **Series**.\n",
    "- **Selecting multiple columns:** Use a list of column names inside the brackets. This returns a **DataFrame**.\n",
    "- **Slicing rows:** You can slice rows using integer-based indexing, just like with a Python list. The slice is based on the row's position.\n",
    "\n",
    "#### Label-based Selection with `.loc[]`\n",
    "\n",
    "`.loc[]` is the primary method for **label-based** indexing. It selects data based on row and column labels. The syntax is `df.loc[row_label, column_label]`. The start and end labels are **inclusive**.\n",
    "\n",
    "- **Selecting by label:** Use a single label or a list of labels.\n",
    "- **Slicing with labels:** `.loc[]` can slice both rows and columns.\n",
    "- **Boolean indexing:** `.loc[]` is excellent for filtering data based on a condition.\n",
    "\n",
    "#### Position-based Selection with `.iloc[]`\n",
    "\n",
    "`.iloc[]` is for **integer-location based** indexing. It selects data based on the integer position of rows and columns, similar to NumPy arrays. The syntax is `df.iloc[row_position, column_position]`. The end position is **exclusive**, just like Python slicing.\n",
    "\n",
    "- **Selecting by position:** Use an integer or a list of integers.\n",
    "- **Slicing with positions:** Slicing works exactly like Python lists.\n",
    "\n",
    "#### Summary of Differences\n",
    "\n",
    "| Method | Syntax | Selection Type | End Point |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| `[]` | `df[label]` | Column (sometimes rows) | Exclusive (for slicing) |\n",
    "| `.loc[]` | `df.loc[row_labels, col_labels]` | Label-based | Inclusive |\n",
    "| `.iloc[]` | `df.iloc[row_positions, col_positions]` | Position-based | Exclusive |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de133b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame for selection examples:\")\n",
    "print(df)\n",
    "\n",
    "# Select column 'A'\n",
    "series_A = df['A']\n",
    "print(\"\\nColumn 'A':\")\n",
    "print(series_A)\n",
    "\n",
    "# Select columns 'A' and 'C'\n",
    "df_AC = df[['A', 'C']]\n",
    "print(\"\\nColumns 'A' and 'C':\")\n",
    "print(df_AC)\n",
    "\n",
    "# Select rows from index 0 up to (but not including) 2\n",
    "rows_0_to_1 = df[0:2]\n",
    "print(\"\\nRows from index 0 to 1:\")\n",
    "print(rows_0_to_1)\n",
    "\n",
    "# Select row with label 1 and column with label 'B'\n",
    "value = df.loc[1, 'B']\n",
    "print(f\"\\nValue at row label 1 and column 'B': {value}\")\n",
    "\n",
    "# Select all columns for row with label 0\n",
    "row_0 = df.loc[0, :]\n",
    "print(\"\\nAll columns for row with label 0:\")\n",
    "print(row_0)\n",
    "\n",
    "# Select all rows from label 0 to 2, and columns from 'A' to 'C'\n",
    "subset = df.loc[0:2, 'A':'C']\n",
    "print(\"\\nSubset of rows 0 to 2 and columns 'A' to 'C':\")\n",
    "print(subset)\n",
    "\n",
    "# Select all rows where column 'B' is greater than 5\n",
    "filtered_df = df.loc[df['B'] > 5]\n",
    "print(\"\\nRows where column 'B' is greater than 5:\")\n",
    "print(filtered_df)\n",
    "\n",
    "# Select the value in the row at position 1 and column at position 1\n",
    "print(f\"\\nValue at row position 1 and column position 1: {value}\")\n",
    "\n",
    "# Select all rows at positions 0 and 2, and all columns\n",
    "rows_0_2 = df.iloc[[0, 2], :]\n",
    "print(\"\\nRows at positions 0 and 2:\")\n",
    "print(rows_0_2)\n",
    "\n",
    "# Select rows from position 0 up to (but not including) 2, and columns from 1 up to (but not including) 3\n",
    "subset = df.iloc[0:2, 1:3]\n",
    "print(\"\\nSubset of rows 0 to 2 and columns at positions 1 to 3:\")\n",
    "print(subset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2273d",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "\n",
    "When handling missing data in Pandas, you need to first **detect** where the missing values are, then decide whether to **drop** the rows or columns with missing data, or **fill** them with new values.\n",
    "\n",
    "#### Detecting Missing Data\n",
    "\n",
    "The first step is to identify where missing values exist. Pandas represents missing data with `NaN` (Not a Number) for numerical data and `None` or `NaN` for other data types.\n",
    "\n",
    "- **`.isnull()` or `.isna()`**: These methods return a DataFrame of boolean values where `True` indicates a missing value.\n",
    "* **`.isnull().sum()`**: This is the most common way to get a count of missing values per column.\n",
    "\n",
    "#### Dropping Missing Data\n",
    "\n",
    "If the number of missing values is small, or if dropping them won't significantly impact your analysis, you can simply remove the rows or columns.\n",
    "\n",
    "- **`.dropna()`**: This method removes rows or columns that contain missing values.\n",
    "- **Dropping rows**: The default behavior of `.dropna()` is to drop any row that has **at least one** missing value.\n",
    "- **Dropping columns**: To drop columns with missing values, set the `axis` parameter to `1` (or `'columns'`).\n",
    "- **Controlling the drop**: You can use the `how` parameter to control the dropping behavior.\n",
    "    - `how='any'` (default): Drop if **any** value is `NaN`.\n",
    "    - `how='all'`: Drop only if **all** values in a row/column are `NaN`.\n",
    "\n",
    "#### Filling Missing Data\n",
    "\n",
    "Often, you don't want to lose data by dropping rows or columns. **Filling** or **imputing** missing values is a common solution.\n",
    "\n",
    "- **`.fillna()`**: This is the main function for filling missing values.\n",
    "- **Filling with a single value**: You can fill all `NaN` values with a specific value.\n",
    "- **Filling with a statistical measure**: A common approach is to fill missing values with the mean, median, or mode of that column.\n",
    "- **Forward-fill (`.ffill()`) or Backward-fill (`.bfill()`)**: These methods fill missing values using the value from the previous (`.ffill()`) or next (`.bfill()`) valid observation. This is useful for time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = {'A': [1, 2, np.nan], 'B': [4, np.nan, 6], 'C': [7, 8, 9]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame with missing values:\")\n",
    "print(df)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nDataFrame missing values:\")\n",
    "print(df.isnull())\n",
    "\n",
    "# Get the count of missing values for each column\n",
    "print(f\"\\nThe count of missing values for each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop any row with a missing value\n",
    "df_dropped_rows = df.dropna()\n",
    "print(\"\\nDataFrame after dropping rows with any missing values:\")\n",
    "print(df_dropped_rows)\n",
    "\n",
    "# Drop any column with a missing value\n",
    "df_dropped_cols = df.dropna(axis=1)\n",
    "print(\"\\nDataFrame after dropping columns with any missing values:\")\n",
    "print(df_dropped_cols)\n",
    "\n",
    "# Drop rows where all values are NaN\n",
    "df_dropped_all = df.dropna(how='all')\n",
    "print(\"\\nDataFrame after dropping rows where all values are missing:\")\n",
    "print(df_dropped_all)\n",
    "\n",
    "# Fill all missing values with 0\n",
    "df_filled_zero = df.fillna(0)\n",
    "print(\"\\nDataFrame after filling missing values with 0:\")\n",
    "print(df_filled_zero)\n",
    "\n",
    "# Fill missing values in column 'B' with the mean of that column\n",
    "mean_b = df['B'].mean()\n",
    "df_filled_mean = df['B'].fillna(mean_b)\n",
    "print(\"\\nDataFrame after filling missing values in column 'B' with the mean of that column:\")\n",
    "print(df_filled_mean)\n",
    "\n",
    "# This will only fill the missing value in column B, not A\n",
    "df_filled_with_means = df.fillna(df.mean(numeric_only=True))\n",
    "print('\\nDataFrame after filling missing values with column means:')\n",
    "print(df_filled_with_means)\n",
    "\n",
    "# Forward-fill missing values\n",
    "df_ffill = df.ffill()\n",
    "print('\\nDataFrame after forward-filling missing values:')\n",
    "print(df_ffill)\n",
    "\n",
    "# Backward-fill missing values\n",
    "df_bfill = df.bfill()\n",
    "print('\\nDataFrame after backward-filling missing values:')\n",
    "print(df_bfill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c294f59",
   "metadata": {},
   "source": [
    "### Data Manipulation\n",
    "\n",
    "#### Adding New Columns\n",
    "\n",
    "You can add a new column to a `DataFrame` by assigning a `Series` or a list to a new column name.\n",
    "\n",
    "- **From a single value**: Assigning a single value to a new column will populate every row with that value.\n",
    "- **From an existing column**: A common practice is to create a new column based on calculations from one or more existing columns.\n",
    "- **Using `.assign()`**: The `.assign()` method is an alternative way to add new columns. It's especially useful for method chaining as it returns a new `DataFrame`.\n",
    "\n",
    "#### Modifying Existing Columns\n",
    "\n",
    "You can modify an existing column by selecting it and then assigning a new value to it, just as you would when adding a new column.\n",
    "\n",
    "- **Direct assignment**: Simply use the column's name to select it and then assign the new values.\n",
    "- **Conditional modification**: Use boolean indexing with `.loc[]` to modify values based on a condition.\n",
    "\n",
    "#### Dropping Columns\n",
    "\n",
    "You can remove one or more columns using the `.drop()` method. The `axis=1` parameter is crucial as it specifies that you are dropping a column rather than a row.\n",
    "\n",
    "- **Dropping a single column**: Pass the column name as a string.\n",
    "- **Dropping multiple columns**: Pass a list of column names.\n",
    "\n",
    "To drop columns from the original `DataFrame` without creating a new one, use the `inplace=True` parameter.\n",
    "\n",
    "#### Renaming Columns\n",
    "\n",
    "The `.rename()` method is used to change the names of one or more columns. It takes a dictionary where keys are the old column names and values are the new names.\n",
    "\n",
    "- **Renaming one or more columns**:\n",
    "\n",
    "Just like with `.drop()`, you can use `inplace=True` to modify the original `DataFrame`.\n",
    "\n",
    "- **Using a function**: You can also pass a function to `rename` to perform a uniform operation on all column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'A': [1, 2, 3], 'B': [4, 5, 6]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Add a new column 'C' with a constant value\n",
    "df['C'] = 10\n",
    "print(\"\\nDataFrame after adding column 'C':\")\n",
    "print(df)\n",
    "\n",
    "# Add a new column 'D' which is the sum of columns 'A' and 'B'\n",
    "df['D'] = df['A'] + df['B']\n",
    "print(\"\\nDataFrame after adding column 'D':\")\n",
    "print(df)\n",
    "\n",
    "# Add a new column 'E' using the assign method\n",
    "df_new = df.assign(E=lambda x: x['A'] * 2)\n",
    "print(\"\\nDataFrame after adding column 'E':\")\n",
    "print(df_new)\n",
    "\n",
    "# Rename columns to lowercase\n",
    "df_new.columns = ['a', 'b', 'c', 'd', 'e']\n",
    "print(\"\\nDataFrame after renaming columns to lowercase:\")\n",
    "print(df_new)\n",
    "\n",
    "# Modify column 'A' by multiplying its values by 10\n",
    "df['A'] = df['A'] * 10\n",
    "print(\"\\nDataFrame after modifying column 'A':\")\n",
    "print(df)\n",
    "\n",
    "# Change values in column 'B' to 99 where 'A' is greater than 15\n",
    "df.loc[df['A'] > 15, 'B'] = 99\n",
    "print(\"\\nDataFrame after modifying column 'B':\")\n",
    "print(df)\n",
    "\n",
    "# Drop column 'B'\n",
    "df_no_B = df.drop('B', axis=1)\n",
    "print(\"\\nDataFrame after dropping column 'B':\")\n",
    "print(df_no_B)\n",
    "\n",
    "# Drop columns 'A' and 'C'\n",
    "df_no_AC = df.drop(['A', 'C'], axis=1)\n",
    "print(\"\\nDataFrame after dropping columns 'A' and 'C':\")\n",
    "print(df_no_AC)\n",
    "\n",
    "# Drop 'A' and 'B' permanently from the original DataFrame\n",
    "df.drop(['A', 'B'], axis=1, inplace=True)\n",
    "print(\"\\nDataFrame after dropping columns 'A' and 'B' permanently:\")\n",
    "print(df)\n",
    "\n",
    "# Rename 'A' to 'First' and 'B' to 'Second'\n",
    "df_renamed = df.rename(columns={'A': 'First', 'B': 'Second'})\n",
    "print(\"\\nDataFrame after renaming columns 'A' to 'First' and 'B' to 'Second':\")\n",
    "print(df_renamed)\n",
    "\n",
    "# Convert all column names to lowercase\n",
    "df.rename(columns=str.lower, inplace=True)\n",
    "print(\"\\nDataFrame after converting column names to lowercase using function:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279e32e",
   "metadata": {},
   "source": [
    "### Grouping and Aggregation\n",
    "\n",
    "You can perform grouping and aggregation in pandas using the **`.groupby()`** method, which is a powerful way to summarize data. The process typically involves three steps: \n",
    "\n",
    "1. **split**\n",
    "2. **apply**\n",
    "3. **combine**\n",
    "\n",
    "#### The `groupby()` Method\n",
    "\n",
    "The `groupby()` method is at the core of this process. You use it to split your DataFrame into groups based on one or more columns. The result isn't a DataFrame itself, but a special `GroupBy` object that's ready for an aggregation function.\n",
    "\n",
    "- **Syntax**: `df.groupby('column_to_group_by')` or `df.groupby(['col1', 'col2'])`\n",
    "    - **Split**: This is the grouping step. For example, if you group by the 'City' column, pandas internally creates separate groups for 'New York', 'Chicago', etc.\n",
    "    - **Apply**: Next, you apply an aggregation function to each group. This function calculates a single value for each group, such as the mean, sum, or count.\n",
    "    - **Combine**: Finally, pandas combines the results from each group into a new DataFrame or Series.\n",
    "\n",
    "#### Common Aggregation Functions\n",
    "\n",
    "After grouping, you apply an aggregation function to the `GroupBy` object. Some of the most common functions are:\n",
    "\n",
    "- **`.sum()`**: Computes the sum of values within each group.\n",
    "- **`.mean()`**: Calculates the average of values in each group.\n",
    "- **`.count()`**: Counts the number of non-null values in each group.\n",
    "- **`.min()` and `.max()`**: Finds the minimum and maximum values in each group.\n",
    "- **`.size()`**: Counts the total number of rows (including nulls) in each group.\n",
    "- **`.agg()`**: Allows you to apply multiple aggregation functions at once.\n",
    "\n",
    "#### Step-by-Step Examples\n",
    "\n",
    "Let's use a simple DataFrame to demonstrate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'City': ['New York', 'New York', 'Chicago', 'Chicago', 'New York'],\n",
    "        'Product': ['A', 'B', 'A', 'A', 'C'],\n",
    "        'Sales': [100, 150, 200, 50, 300]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f5873",
   "metadata": {},
   "source": [
    "**1. Grouping by a single column and calculating the sum:**\n",
    "\n",
    "To find the total sales for each city, you group by 'City' and then apply the `.sum()` function to the 'Sales' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604de1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'City' and sum the 'Sales' for each group\n",
    "city_sales = df.groupby('City')['Sales'].sum()\n",
    "print(\"\\nTotal sales per city:\")\n",
    "print(city_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ad3db",
   "metadata": {},
   "source": [
    "**2. Grouping by multiple columns:**\n",
    "\n",
    "You can group by more than one column to get more granular results. For example, to find the total sales for each product in each city, you group by both 'City' and 'Product'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ba560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'City' and 'Product' and get the mean of 'Sales'\n",
    "avg_sales = df.groupby(['City', 'Product'])['Sales'].mean()\n",
    "print(\"\\nAverage sales per city and product:\")\n",
    "print(avg_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb5f8fc",
   "metadata": {},
   "source": [
    "**3. Applying multiple aggregations with `.agg()`:**\n",
    "\n",
    "The `.agg()` method is highly flexible and lets you apply multiple aggregation functions at once, even to different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e53e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'City' and get the sum and mean of 'Sales'\n",
    "city_stats = df.groupby('City').agg(\n",
    "    total_sales=('Sales', 'sum'),\n",
    "    average_sales=('Sales', 'mean'),\n",
    "    number_of_transactions=('Sales', 'count')\n",
    ")\n",
    "print(\"\\nMultiple aggregations per city:\")\n",
    "print(city_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ad9ed",
   "metadata": {},
   "source": [
    "The syntax `total_sales=('Sales', 'sum')` creates a new column called 'total\\_sales' with the sum of the 'Sales' column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a78e2b2",
   "metadata": {},
   "source": [
    "### Merging and Joining DataFrames\n",
    "\n",
    "Merging and joining DataFrames is a fundamental operation in pandas for combining data from different sources. The primary methods for this are **`pd.merge()`** and **`DataFrame.join()`**.\n",
    "\n",
    "#### Understanding Merges (`pd.merge()`)\n",
    "\n",
    "The `pd.merge()` function is a versatile tool for combining two DataFrames based on a common column or index. It's similar to SQL `JOIN` operations.\n",
    "\n",
    "The key parameters of `pd.merge()` are:\n",
    "\n",
    "- **`left` and `right`**: The two DataFrames you want to merge.\n",
    "- **`on`**: The column name(s) to join on. If the column names are different in the two DataFrames, you can use `left_on` and `right_on`.\n",
    "- **`how`**: The type of merge to perform. This is the most crucial parameter. The options are:\n",
    "    - **`'inner'`** (default): Returns only the rows where the key is present in **both** DataFrames. This is the most common type of merge.\n",
    "    - **`'outer'`**: Returns all rows from both DataFrames. Where keys don't match, `NaN` is filled in.\n",
    "    - **`'left'`**: Returns all rows from the **left** DataFrame, and matching rows from the right. `NaN` is filled for rows in the left DataFrame that have no match in the right.\n",
    "    - **`'right'`**: Returns all rows from the **right** DataFrame, and matching rows from the left. `NaN` is filled for rows in the right DataFrame that have no match in the left.\n",
    "\n",
    "#### Example: Inner and Outer Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c53202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two DataFrames\n",
    "df1 = pd.DataFrame({'key': ['A', 'B', 'C', 'D'],\n",
    "                    'value1': [1, 2, 3, 4]})\n",
    "\n",
    "df2 = pd.DataFrame({'key': ['A', 'B', 'E', 'F'],\n",
    "                    'value2': [5, 6, 7, 8]})\n",
    "\n",
    "print(\"DataFrame 1:\")\n",
    "print(df1)\n",
    "print(\"\\nDataFrame 2:\")\n",
    "print(df2)\n",
    "\n",
    "# Perform an inner merge\n",
    "merged_df = pd.merge(df1, df2, on='key', how='inner')\n",
    "\n",
    "print(\"\\nInner Merge:\")\n",
    "print(merged_df)\n",
    "\n",
    "# Perform an outer merge\n",
    "merged2_df = pd.merge(df1, df2, on='key', how='outer')\n",
    "print(\"\\nOuter Merge:\")\n",
    "print(merged2_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d6e2e",
   "metadata": {},
   "source": [
    "**Explanation**: The result of the inner merge only includes keys 'A' and 'B' because they are the only ones present in both DataFrames. The result of the outer merge includes all the keys of both datasets, and assigns a missing value (NaN) to the features (columns) that do not have a value in the original DataFrames\n",
    "\n",
    "#### Understanding Joins (`DataFrame.join()`)\n",
    "\n",
    "The `DataFrame.join()` method is a convenient way to combine two DataFrames on their **indexes**. It's primarily used when the keys you want to join on are the DataFrame indexes.\n",
    "\n",
    "- **Syntax**: `left_df.join(right_df)`\n",
    "\n",
    "**Example: Join on Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two DataFrames\n",
    "df1 = pd.DataFrame({'key': ['A', 'B', 'C', 'D'],\n",
    "                    'value1': [1, 2, 3, 4]})\n",
    "\n",
    "df2 = pd.DataFrame({'key': ['A', 'B', 'E', 'F'],\n",
    "                    'value2': [5, 6, 7, 8]})\n",
    "print(\"DataFrame 1:\")\n",
    "print(df1)\n",
    "print(\"\\nDataFrame 2:\")\n",
    "print(df2)\n",
    "\n",
    "# Create two DataFrames with different indexes\n",
    "df1_indexed = df1.set_index('key')\n",
    "df2_indexed = df2.set_index('key')\n",
    "\n",
    "# Perform a left join on the index\n",
    "joined_df = df1_indexed.join(df2_indexed, how='left')\n",
    "\n",
    "print(\"\\nLeft Join:\")\n",
    "print(joined_df)\n",
    "\n",
    "# Perform a right join on the index\n",
    "joined_df_right = df1_indexed.join(df2_indexed, how='right')\n",
    "print(\"\\nRight Join:\")\n",
    "print(joined_df_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36ff8f",
   "metadata": {},
   "source": [
    "**Explanation**: The `join()` method defaults to a left join. Since `df1_indexed` has keys 'A', 'B', 'C', and 'D', the result will include all of these. The `value2` for keys 'C' and 'D' will be `NaN` because there is no match in `df2_indexed`. In the case of the right join, the keys are 'A', 'B', 'E', and 'F', thus 'C' and 'D' get dropped.\n",
    "\n",
    "#### Merge vs. Join: Which to Use?\n",
    "\n",
    "- **`pd.merge()`**: Use `merge()` when you need to join on specific columns, especially when those columns are not the index. It is the more flexible and general-purpose function.\n",
    "- **`DataFrame.join()`**: Use `join()` for joining on the DataFrame's index. It's often cleaner and more concise for these specific use cases. You can also join on a column by setting the `on` parameter, but `merge()` is generally preferred for this task.\n",
    "\n",
    "\n",
    "### Applying Functions\n",
    "\n",
    "The `.apply()` method in pandas is a powerful and flexible tool for applying a function along an axis of a DataFrame or to the elements of a Series. It's often used when a standard vectorized operation isn't available or is too complex.\n",
    "\n",
    "#### `.apply()` on a Series\n",
    "\n",
    "When you use `.apply()` on a **Series**, the function is applied to each individual element in that Series.\n",
    "\n",
    "**Example**: Let's create a Series and apply a simple function to it. We'll use a lambda function to convert temperatures from Celsius to Fahrenheit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea9cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series of temperatures in Celsius\n",
    "temps_celsius = pd.Series([10, 20, 30, 40])\n",
    "print(\"Temperatures in Celsius:\")\n",
    "print(temps_celsius)\n",
    "\n",
    "# Define a function to convert Celsius to Fahrenheit\n",
    "def c_to_f(celsius):\n",
    "    return (celsius * 9/5) + 32\n",
    "\n",
    "# Apply the function to each element of the Series\n",
    "temps_fahrenheit = temps_celsius.apply(c_to_f)\n",
    "\n",
    "print(\"\\nTemperatures in Fahrenheit:\")\n",
    "print(temps_fahrenheit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2844e",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation**: The `.apply()` method takes the `c_to_f` function and applies it to each value in `temps_celsius`, creating a new Series with the converted temperatures.\n",
    "\n",
    "You can also use a **lambda function** for more concise operations:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a lambda function to perform the same conversion\n",
    "temps_fahrenheit_lambda = temps_celsius.apply(lambda x: (x * 9/5) + 32)\n",
    "print(\"\\nTemperatures in Fahrenheit using lambda:\")\n",
    "print(temps_fahrenheit_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1d720",
   "metadata": {},
   "source": [
    "This is a very common pattern in pandas for simple element-wise transformations.\n",
    "\n",
    "#### `.apply()` on a DataFrame\n",
    "\n",
    "When you use `.apply()` on a **DataFrame**, the function is applied to each row or each column. You specify the axis along which the function should be applied.\n",
    "\n",
    "- **`axis=0`**: Applies the function to each **column**. The function will receive a Series (the column) as input.\n",
    "- **`axis=1`**: Applies the function to each **row**. The function will receive a Series (the row) as input.\n",
    "\n",
    "Example: Applying to each row (`axis=1`)\n",
    "\n",
    "Let's use a DataFrame of student scores and calculate their average grade for each row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41174b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Math': [80, 90, 75],\n",
    "        'Science': [85, 95, 80],\n",
    "        'English': [90, 85, 70]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Define a function to calculate the average of a row\n",
    "def calculate_average(row):\n",
    "    return (row['Math'] + row['Science'] + row['English']) / 3\n",
    "\n",
    "# Apply the function to each row\n",
    "df['Average'] = df.apply(calculate_average, axis=1)\n",
    "print(\"\\nDataFrame with average grades:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b385b",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation**: By specifying `axis=1`, we tell `.apply()` to iterate through each row. The `calculate_average` function receives each row as a Series and can access column values using their labels (e.g., `row['Math']`).\n",
    "\n",
    "#### Example: Applying to each column (`axis=0`)\n",
    "\n",
    "You can use `axis=0` to perform column-wise operations, like finding the minimum score for each subject.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da119512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a function to each column to find the minimum value\n",
    "min_scores = df[['Math', 'Science', 'English']].apply(min, axis=0)\n",
    "\n",
    "print(\"Minimum score for each subject:\")\n",
    "print(min_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b95560",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation**: In this case, `.apply()` passes each column as a Series to the built-in `min()` function, returning the minimum value for 'Math', 'Science', and 'English' respectively.\n",
    "\n",
    "While `.apply()` is very versatile, it can be slower than built-in vectorized pandas operations.  For simple arithmetic (like `df['A'] + df['B']`), it is always better to use the direct vectorized operation. Use `.apply()` when your function requires custom logic or access to multiple columns on a row-by-row basis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9cb3b7",
   "metadata": {},
   "source": [
    "### Pivoting and Melting\n",
    "\n",
    "Pivoting and melting are two common data restructuring operations in pandas. They are inverse operations, used to transform data between \"long\" and \"wide\" formats.\n",
    "\n",
    "#### Pivoting (Wide Format)\n",
    "\n",
    "**Pivoting** is the process of reshaping data from a \"long\" or normalized format to a \"wide\" format. It takes a unique value from one column and makes it a new column header. Think of it like creating a pivot table in Excel. The main function for this is **`DataFrame.pivot()`**.\n",
    "\n",
    "The `pivot()` function takes three arguments:\n",
    "\n",
    "- **`index`**: The column to use as the new DataFrame index.\n",
    "- **`columns`**: The column whose unique values will become the new column headers.\n",
    "- **`values`**: The column(s) whose values will populate the new DataFrame.\n",
    "\n",
    "Example\n",
    "\n",
    "Imagine you have sales data for different products over a few months. This data is in a long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Long format DataFrame\n",
    "df_long = pd.DataFrame({\n",
    "    'Month': ['Jan', 'Jan', 'Feb', 'Feb', 'Mar', 'Mar'],\n",
    "    'Product': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'Sales': [100, 150, 120, 160, 130, 170]\n",
    "})\n",
    "\n",
    "print(\"Original (Long) DataFrame:\")\n",
    "print(df_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47865033",
   "metadata": {},
   "source": [
    "To analyze the sales of each product side-by-side, you can pivot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to wide format\n",
    "df_wide = df_long.pivot(index='Month', columns='Product', values='Sales')\n",
    "\n",
    "print(\"\\nPivoted (Wide) DataFrame:\")\n",
    "print(df_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27024597",
   "metadata": {},
   "source": [
    "The `Month` column becomes the new index, and the unique values from the `Product` column (`A` and `B`) become the new column names. The corresponding `Sales` values fill the table.\n",
    "\n",
    "#### Melting (Long Format)\n",
    "\n",
    "**Melting** is the inverse operation of pivoting. It reshapes a DataFrame from a \"wide\" format to a \"long\" format. This is useful when you have data where columns represent variables, and you want to convert these columns into rows. The main function for this is **`pd.melt()`**.\n",
    "\n",
    "The `melt()` function takes two main arguments:\n",
    "\n",
    "- **`id_vars`**: The column(s) to remain as identifier variables. These will not be melted.\n",
    "- **`value_vars`**: The column(s) to melt. These columns' names will become a new variable column, and their values will become a new value column.\n",
    "\n",
    "Example\n",
    "\n",
    "Using the `df_wide` DataFrame from the previous example, let's melt it back to the original long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5573dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the wide DataFrame back to long format\n",
    "df_melted = pd.melt(df_wide.reset_index(),\n",
    "                    id_vars=['Month'],\n",
    "                    value_vars=['A', 'B'],\n",
    "                    var_name='Product',\n",
    "                    value_name='Sales')\n",
    "\n",
    "print(\"\\nMelted (Long) DataFrame:\")\n",
    "print(df_melted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a721f795",
   "metadata": {},
   "source": [
    "We use `df_wide.reset_index()` to turn the `Month` index back into a column. Then, the `Product` column is created from the melted column headers (`A`, `B`), and the `Sales` column is created from their corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78e677",
   "metadata": {},
   "source": [
    "### Time Series Functionality\n",
    "\n",
    "Pandas offers robust functionality for working with time-series data, from parsing dates to resampling data. The primary objects for this are the `DatetimeIndex`, `PeriodIndex`, and `TimedeltaIndex`.\n",
    "\n",
    "#### Creating a Time-Series Index\n",
    "\n",
    "The first step is to ensure your DataFrame has a proper time-series index.\n",
    "\n",
    "- **Using `pd.to_datetime()`**: This function converts a column with string or integer dates into a `datetime` object, which is essential for time-series operations.\n",
    "- **Setting the index**: To unlock time-series functionality, set the datetime column as the DataFrame's index.\n",
    "- **Slicing by date**: You can use strings to slice by year, month, or day.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7787be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'date': ['2023-01-01', '2023-01-02', '2023-01-03',\n",
    "                            '2024-01-01', '2024-01-02', '2024-01-03',],\n",
    "                'value': [10, 15, 20, 25, 30, 35]})\n",
    "\n",
    "# Convert the 'date' column to a datetime object\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "print(\"\\nDataFrame with 'date' column as datetime:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Set the 'date' column as the DataFrame's index\n",
    "df.set_index('date', inplace=True)\n",
    "print(\"\\nDataFrame with 'date' as index:\")\n",
    "print(df)\n",
    "\n",
    "# Select all data from a specific year\n",
    "df_2023 = df.loc['2023']\n",
    "print(\"\\nData for the year 2023:\")\n",
    "print(df_2023)\n",
    "\n",
    "# Select all data from a specific month\n",
    "df_jan = df.loc['2023-01']\n",
    "print(\"\\nData for January 2023:\")\n",
    "print(df_jan)\n",
    "\n",
    "# Select data for a specific date range\n",
    "df_slice = df.loc['2023-01-01':'2023-01-03']\n",
    "print(\"\\nData from 2023-01-01 to 2023-01-03:\")\n",
    "print(df_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91ec53",
   "metadata": {},
   "source": [
    "### Resampling Time-Series Data\n",
    "\n",
    "**Resampling** is the process of changing the frequency of your time-series data. It is a powerful tool for aggregation.\n",
    "\n",
    "- **Downsampling**: Converting high-frequency data to low-frequency data (e.g., from daily to monthly). When downsampling, you must provide an aggregation function like `mean()`, `sum()`, or `first()`.\n",
    "- **Upsampling**: Converting low-frequency data to high-frequency data (e.g., from monthly to daily). When upsampling, you must decide how to fill the new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a daily time series\n",
    "daily_data = pd.DataFrame({'value': np.random.rand(365)},\n",
    "                          index=pd.date_range(start='2023-01-01', periods=365, freq='D'))\n",
    "print(daily_data)\n",
    "\n",
    "# Resample from daily to monthly, taking the mean of each month\n",
    "monthly_mean = daily_data.resample('ME').mean()\n",
    "print(\"Downsampled to monthly means:\")\n",
    "print(monthly_mean.head())\n",
    "\n",
    "# Upsample from monthly to weekly\n",
    "weekly_data = monthly_mean.resample('W').asfreq()\n",
    "print(\"\\nUpsampled to weekly frequency (with NaNs):\")\n",
    "print(weekly_data)\n",
    "\n",
    "# Upsample and fill missing values with forward fill or backward fill\n",
    "weekly_filled_f = monthly_mean.resample('W').ffill()\n",
    "weekly_filled_b = monthly_mean.resample('W').bfill()\n",
    "print(\"\\nUpsampled to weekly frequency (forward fill):\")\n",
    "print(weekly_filled_f)\n",
    "print(\"\\nUpsampled to weekly frequency (backward fill):\")\n",
    "print(weekly_filled_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd4740",
   "metadata": {},
   "source": [
    "#### `Timedelta` Objects\n",
    "\n",
    "A `Timedelta` represents a duration, the difference between two dates or times.\n",
    "\n",
    "- **Creating a `Timedelta`**: You can create a `Timedelta` by subtracting two `datetime` objects.\n",
    "- **Adding/Subtracting**: You can add or subtract `Timedelta` objects to/from `datetime` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee36232",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.to_datetime('2023-01-01 10:00:00')\n",
    "end = pd.to_datetime('2023-01-01 11:30:00')\n",
    "\n",
    "duration = end - start\n",
    "print(\"Timedelta object:\")\n",
    "print(duration)\n",
    "\n",
    "# Add a Timedelta to a datetime\n",
    "new_time = start + pd.to_timedelta('2 hours')\n",
    "print(\"\\nNew time after adding 2 hours:\")\n",
    "print(new_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5705a",
   "metadata": {},
   "source": [
    "### Advanced Operations\n",
    "\n",
    "Pandas can do even more than what I have shown so far. Here is a quick high-level highlight of handling categorical data, working with MultiIndex objects, and plotting directly from DataFrames.\n",
    "\n",
    "#### Categorical Data\n",
    "\n",
    "**Categorical data** is a pandas data type that represents a variable with a fixed, limited number of possible values (categories). It's more memory-efficient than object or string data, and can improve performance for certain operations.\n",
    "\n",
    "You can convert a column to the categorical type using `astype('category')`.\n",
    "\n",
    "Benefits of Categorical Data include\n",
    "\n",
    "  * **Memory Efficiency**: Storing categories as integer codes rather than repeating strings can save a significant amount of memory, especially with large datasets.\n",
    "  * **Performance**: Operations like `groupby()` can be much faster with categorical data.\n",
    "  * **Ordering**: You can define a specific order for your categories, which is useful for sorting and plotting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342533ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'product_id': [1, 2, 3, 4],\n",
    "                   'category': ['shoes', 'clothing', 'shoes', 'accessories']})\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "# Convert the 'category' column to the 'category' data type\n",
    "df['category'] = df['category'].astype('category')\n",
    "print(\"\\nDataFrame after converting 'category' to categorical type:\")\n",
    "print(df.info())\n",
    "\n",
    "# Define a specific order for the categories\n",
    "df['category'] = pd.Categorical(df['category'], categories=['clothing', 'shoes', 'accessories'], ordered=True)\n",
    "print(\"\\nDataFrame after defining category order:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nCategory codes:\")\n",
    "print(df['category'].cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4b320",
   "metadata": {},
   "source": [
    "#### MultiIndex\n",
    "\n",
    "A **MultiIndex** is a hierarchical index that allows you to have multiple levels of labels on an axis (rows or columns). This is powerful for handling complex, multi-dimensional data, often created after using `groupby()` or `pivot_table()`.\n",
    "\n",
    "- You can create a MultiIndex by setting multiple columns as the index.\n",
    "- Slicing and selecting data with a MultiIndex can be done using a tuple.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'city': ['NY', 'NY', 'SF', 'SF'],\n",
    "        'year': [2020, 2021, 2020, 2021],\n",
    "        'sales': [100, 150, 200, 250]}\n",
    "\n",
    "df = pd.DataFrame(data).set_index(['city', 'year'])\n",
    "print(\"MultiIndex DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Select data for 'SF' in 2020\n",
    "sales_sf_2020 = df.loc[('SF', 2020)]\n",
    "print(\"\\nSales in SF for 2020:\")\n",
    "print(sales_sf_2020)\n",
    "\n",
    "# Select all years for 'NY'\n",
    "all_years_ny = df.loc['NY']\n",
    "print(\"\\nAll years for NY:\")\n",
    "print(all_years_ny)\n",
    "\n",
    "# Use a slice to select a range of years for a specific city\n",
    "range_years = df.loc[('NY', 2020):('NY', 2021)]\n",
    "print(\"\\nRange of years for NY from 2020 to 2021:\")\n",
    "print(range_years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e4f71",
   "metadata": {},
   "source": [
    "#### Plotting\n",
    "\n",
    "Pandas has a built-in plotting functionality that is a wrapper around the **Matplotlib** library. This allows for quick and convenient visualization of your data directly from a DataFrame or Series.\n",
    "\n",
    "To use the plotting functions, you must have Matplotlib installed. The syntax to plot from a data frame is: \n",
    "\n",
    "- **Syntax**: `df.plot(kind='plot_type')`\n",
    "\n",
    "Common Plot Types\n",
    "\n",
    "You can specify the plot type using the `kind` parameter or by calling a specific plot method.\n",
    "\n",
    "- `kind='line'` or `df.plot.line()`: Line plot\n",
    "- `kind='bar'` or `df.plot.bar()`: Bar plot\n",
    "- `kind='hist'` or `df.plot.hist()`: Histogram\n",
    "- `kind='scatter'` or `df.plot.scatter(x='A', y='B')`: Scatter plot\n",
    "- `kind='box'` or `df.plot.box()`: Box plot\n",
    "\n",
    "\n",
    "Pandas' plotting functions are great for quick exploratory data analysis, but for more customization and complex plots, it's recommended to use Matplotlib or Seaborn directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for plotting\n",
    "df = pd.DataFrame(np.random.randn(10, 4),\n",
    "                  index=pd.date_range('2023-01-01', periods=10),\n",
    "                  columns=['A', 'B', 'C', 'D'])\n",
    "\n",
    "# Plot a line chart\n",
    "df.plot(kind='line', title='Line Plot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a95bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot\n",
    "df.plot(kind='bar', title='Bar Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f408d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of column 'A'\n",
    "df['A'].plot(kind='hist', title='Histogram of A')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis3803",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
