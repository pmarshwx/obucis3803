{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8174e519",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **Fall 2025 &mdash; CIS 3803<br>Introduction to Data Science**\n",
    "### Week 5 Homework: Data Acquisition, Cleaning, and Processing\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5f0a6",
   "metadata": {},
   "source": [
    "**Assigned Date:** 29 September 2025  \n",
    "**Due Date:** 06 October 2025 @ 6 PM CT\n",
    "\n",
    "Please upload your Jupyter Notebook and necessary data files to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb0a26",
   "metadata": {},
   "source": [
    "## Homework Assignment: Data Acquisition, Cleaning, and Processing \n",
    "\n",
    "**Instructions:**\n",
    "Rename this Jupyter Notebook `yourlastname_week05_homework.ipynb`. For this assignment, you will work with a provided sales dataset. Complete each section by writing the required code in separate cells. Add brief comments to explain your process. A blank code cell is provided after each section that requires code. A markdown cell is provided after each code block and the devotional section.\n",
    "\n",
    "### **Overview**\n",
    "This assignment is designed to solidify your understanding of the initial phases of the Data Science Lifecycle: **Data Acquisition** and **Data Cleaning/Transformation**. You will use the `requests` and `BeautifulSoup` libraries to acquire data and then use **Pandas DataFrames** to clean, transform, and integrate it, specifically focusing on handling missing values, standardizing data, and feature engineering.\n",
    "\n",
    "*****\n",
    "\n",
    "### **Part 1: Data Acquisition (API and Web Scraping)**\n",
    "\n",
    "#### **Task 1A: Structured Data Acquisition (API)**\n",
    "Use the `requests` library and the publicly available **JSONPlaceholder API** (as shown in the lecture) to retrieve a complete list of all **posts** (not users).\n",
    "\n",
    "1.  **Fetch Data:** Make a `GET` request to the appropriate API endpoint to get all post records.\n",
    "2.  **Process:** Convert the resulting JSON data into a Pandas DataFrame named `df_posts`.\n",
    "3.  **Validate:** The final DataFrame should have 100 rows and 4 columns (`userId`, `id`, `title`, `body`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eedd17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67135f30",
   "metadata": {},
   "source": [
    "#### **Task 1B: Unstructured Data Acquisition (Web Scraping)**\n",
    "Use the `requests` and `BeautifulSoup` libraries to scrape the first two pages of the sample book website: `http://books.toscrape.com/`.\n",
    "\n",
    "1.  **Scrape:** Write a loop or two sequential requests to fetch the HTML content for `http://books.toscrape.com/catalogue/page-1.html` and `http://books.toscrape.com/catalogue/page-2.html`.\n",
    "2.  **Extract:** For every book on both pages (40 books total), extract the following two pieces of information:\n",
    "    * **Title** (From the `title` attribute of the `<a>` tag inside the `<h3>`).\n",
    "    * **Price** (The text content from the `<p>` tag with the class `price_color`). *Hint: You will need to remove the currency symbol '£'.*\n",
    "3.  **Combine:** Create a single DataFrame named `df_books` with two columns: `Title` and `Price_USD`.\n",
    "4.  **Clean:** Ensure the `Price_USD` column is a numeric data type (`float`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a188500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff762732",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "### **Part 2: Data Cleaning & Feature Engineering**\n",
    "\n",
    "#### **Task 2A: Cleaning and Transformation on `df_posts`**\n",
    "The `df_posts` DataFrame is very clean, so you will intentionally introduce errors to practice cleaning techniques:\n",
    "\n",
    "1.  **Introduce Errors:** Randomly select 5 rows and manually set the value of the `title` column to `np.nan` (or another missing placeholder like `'N/A'`).\n",
    "2.  **Handle Missing Values (Imputation):** Fill the missing values you created in the `title` column using the **Mode** (most frequent value) of the existing `title` values. *Explain in your submission why using the Mode is appropriate for text data.*\n",
    "3.  **Feature Engineering (Splitting):** Create a new categorical variable called **`First_Word`** by extracting the first word of the text from the `body` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668c713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e2c6e",
   "metadata": {},
   "source": [
    "#### **Task 2B: Cleaning and Feature Engineering on `df_books`**\n",
    "Use the `df_books` DataFrame created in Task 1B.\n",
    "\n",
    "1.  **Handling Outliers (Capping):** Identify the book with the single highest price. Using the concept of **Winsorizing** (or capping, as described in the lecture), replace the price of this single outlier book with the value of the **90th percentile** of all other book prices. *This demonstrates managing a true extreme value.*\n",
    "2.  **Feature Engineering (Binning):** Create a new categorical column called **`Price_Tier`** by binning the `Price_USD` column into three groups: `'Budget'`, `'Standard'`, and `'Premium'`. Use `pd.qcut()` (Quantile-based binning) to ensure each tier has a roughly equal number of books.\n",
    "3.  **Deduplication:** Check your final `df_books` DataFrame for any potential **duplicate records** based on the `Title` column and remove them using `df.drop_duplicates()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34459561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9318e25",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "### **Part 3: Reflection & Application**\n",
    "\n",
    "Write a brief reflection addressing each of the following:\n",
    "\n",
    "1.  **The 80/20 Rule:** Reflect on the time spent on Parts 1 and 2 compared to the time it would take to immediately analyze the data. Does the **Pareto Principle** (80% on cleaning, 20% on analysis) feel accurate based on this exercise?\n",
    "2.  **Data Stewardship:** The course verse is: *\"It is the glory of God to conceal a matter; to search out a matter is the glory of kings.\"* &mdash; *Proverbs 25:2 (NIV)*. In the context of data acquisition and cleaning, what does it mean to be a **diligent steward** of the data, and how does your work in Part 2 reflect the 'search out' process to find clarity or truth in the data? (e.g., how does handling an outlier or a missing value reflect integrity in your search?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2fc60",
   "metadata": {},
   "source": [
    "Reflections Go Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1c7d4",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### **Devotional and Reflection**\n",
    "\n",
    "“Call to me and I will answer you and tell you great and unsearchable things you do not know.”\n",
    "&mdash; Jeremiah 33:3 (NIV)\n",
    "\n",
    "\n",
    "#### **Faith Reflection:** \n",
    "\n",
    "As we begin today’s lecture on data acquisition and cleaning, let us remember that the pursuit of truth &ndash; whether in scripture or in science &ndash; often begins with sorting through the noise. Just as God calls us to seek wisdom and understanding, we are called to be diligent stewards of the data we handle. May our work reflect a deeper desire to uncover truth, serve others with integrity, and glorify God through excellence in even the smallest details.”\n",
    "\n",
    "Where in your own academic or personal life have you had to ‘search out a matter’ — digging through confusion, error, or uncertainty to find clarity or truth? What did that process teach you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24136c",
   "metadata": {},
   "source": [
    "#### **Response:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112f56a",
   "metadata": {},
   "source": [
    "Reflection Goes Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis3803",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
