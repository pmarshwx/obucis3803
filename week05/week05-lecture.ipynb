{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8174e519",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **Fall 2025 &mdash; CIS 3803<br>Introduction to Data Science**\n",
    "### Week 5: Data Acquisition & Cleaning\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5f0a6",
   "metadata": {},
   "source": [
    "**Date:** 29 September 2025  \n",
    "**Time:** 6:00–9:00 PM  \n",
    "**Instructor:** Dr. Patrick T. Marsh  \n",
    "**Course Verse:** “It is the glory of God to conceal a matter; to search out a matter is the glory of kings.” &mdash; *Proverbs 25:2 (NIV)*\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "## **Learning goals**\n",
    "By the end of this lecture, you should be able to:\n",
    "\n",
    "- Explain the role of **Data Acquisition** and **Cleaning** within the Data Science Lifecycle.\n",
    "- Identify common sources of \"**dirty data**\" (Human, System, Measurement errors).\n",
    "- Master the **Pandas DataFrame** as the core data structure for wrangling.\n",
    "- Acquire structured data from web APIs using the `requests` library.\n",
    "- Extract unstructured data from websites using Web Scraping and `BeautifulSoup`.\n",
    "- Implement various techniques for handling **Missing Values** (dropping, imputation).\n",
    "- Validate data by identifying and managing **Implausible Values** and **Outliers**.\n",
    "- Perform **Feature Engineering** tasks, including scaling, binning, and encoding.\n",
    "- Integrate datasets using both **Concatenation (Stacking)** and **Merging (Joining)** techniques.\n",
    "\n",
    "\n",
    "## **Today's Outline**\n",
    "\n",
    "1. **Setting the Stage:** Lifecycle, Pareto's Principle, and Data Sources.\n",
    "1. **Data Acquisition Tutorial:** `requests` and `BeautifulSoup`.\n",
    "1. **The Core Data Structure:** Pandas DataFrames.\n",
    "1. **Phase I: Cleaning the Data**\n",
    "    - Handling Missing Values (Imputation vs. Dropping).\n",
    "    - Validating Content (Implausible Values & Outliers).\n",
    "    - Correcting Structure (Formats & Duplicates).\n",
    "1. **Phase II: Transforming the Data (Feature Engineering)**\n",
    "    - Manipulating Quantitative Variables (Scaling, Binning, Combining).\n",
    "    - Manipulating Categorical Variables (Encoding, Condensing, Splitting).\n",
    "1. **Phase III: Integrating the Data**\n",
    "    - Stacking DataFrames (Concatenation).\n",
    "    - Augmenting DataFrames (Merging/Joins).\n",
    "1. **Glossary & Key Definitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd2a51",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "## **Opening Devotional and Reflection**\n",
    "\n",
    "“Call to me and I will answer you and tell you great and unsearchable things you do not know.”\n",
    "&mdash; Jeremiah 33:3 (NIV)\n",
    "\n",
    "\n",
    "#### **Faith Reflection:** \n",
    "\n",
    "As we begin today’s lecture on data acquisition and cleaning, let us remember that the pursuit of truth &ndash; whether in scripture or in science &ndash; often begins with sorting through the noise. Just as God calls us to seek wisdom and understanding, we are called to be diligent stewards of the data we handle. May our work reflect a deeper desire to uncover truth, serve others with integrity, and glorify God through excellence in even the smallest details.”\n",
    "\n",
    "Where in your own academic or personal life have you had to ‘search out a matter’ — digging through confusion, error, or uncertainty to find clarity or truth? What did that process teach you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05568f54",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "## **Setting the Stage**\n",
    "\n",
    "### **The Data Science Lifecycle: Focus on the Fundamentals**\n",
    "Whether we've explicitly realized it or not, up until now, we've be learning tools to put into our Data Science toolbox. Today we'll start using some of the tools in our toolbox to focus on the first part of the data science lifecycle &mdash; and we'll add more tools to our toolbox along the way.\n",
    "\n",
    "Let's go back to the data science lifecycle flow chart from week 1:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "\n",
    "**Problem $\\longleftrightarrow$ Data Acquistion $\\longleftrightarrow$ Cleaning/Preparation $\\longleftrightarrow$ Exploration/Visualization $\\longleftrightarrow$ Modeling/Inference $\\longleftrightarrow$ Evaluation $\\longleftrightarrow$ Communication/Deployment**\n",
    "\n",
    "</div>\n",
    "\n",
    "We can break down the first half of the life cycle into the following table of questions and take aways:\n",
    "\n",
    "| Phase | Key Question | Take Away |\n",
    "| ------|--------------|-----------|\n",
    "| (Understanding the) Problem | What is the problem we are trying to answer/solve? | The problem guides our goal of Data Acquisition and Cleaning. We only collect and clean data relevant to the question. | \n",
    "| Data Acquisition | Where are the data, and how do we get it? | Collection is the starting point. Without it, the process stops. |\n",
    "| Data Cleaning (Wrangling) | Is the data ready for analysis (consistent, complete accurate)? | This is the main focus. Quality analysis ***requires*** quality data. |\n",
    "| Exploratory Data Analysis | What patterns or insights can we find? | This is the ***reward*** for thorough cleaning. Flawed cleaning leads to flawed insights. Correct insights are crucial for choosing the correct models. Garbage in $\\rightarrow$ Garbage out. (GIGO)|\n",
    "\n",
    "An analogy for this part of the data science lifecycle is cooking:\n",
    "\n",
    "| Phase | Action |\n",
    "|-------|--------|\n",
    "| Question | What do I want to make for dinner? | \n",
    "| Acquisition | Go to the store and purchase ingredients. |\n",
    "| Cleaning/Preparation | Wash the vegetables, trim the fat, discard the rotton items. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6864a5",
   "metadata": {},
   "source": [
    "### **The \"Dirty Data\" Problem: Pareto's Principle**\n",
    "\n",
    "The **Pareto Principle** (also known as the **80/20 Rule**) states that for many outcomes, roughly 80% of consequences come from 20% of the causes. In the context of Data Science, it boils down to 80% of data science is finding, cleaning, and preparing data, while only 20% is spent on actual analysis, modeling, and communicating.\n",
    "\n",
    "#### **Why Data is Dirty: Common Sources**\n",
    "\n",
    "Three common sources of data \"dirt\" comes from:\n",
    "\n",
    "1.  **Human Error (The Typos):**\n",
    "    * **Inconsistent Entry:** Data is often typed in by different people over time.\n",
    "        * *Example:* A column for state names might have entries like `ok`, `Oklahoma`, `OKLA`, and `oklahoma`. All mean the same thing but are treated as four different categories by a computer.\n",
    "    * **Leading/Trailing Whitespace:** Someone hits the space bar after typing a value.\n",
    "        * *Example:* ` 'OBU '` is not the same as `'OBU'`.\n",
    "    * **Placeholder Values:** Data entry personnel use different placeholders for missing data.\n",
    "        * *Example:* Empty fields, `N/A`, `?`, `-999`, or a simple dash `-`. The computer sees all of these as different strings, not as missing values.\n",
    "\n",
    "2.  **System/Integration Error (The Misalignment):**\n",
    "    * **Disparate Systems:** Large organizations pull data from multiple, non-communicating systems (e.g., a student's ID number from the Registrar system doesn't perfectly match the ID number in the Financial Aid system).\n",
    "    * **Format Mismatch:** Data is captured correctly but stored with different types or formats.\n",
    "        * *Example:* One system records student birthdates as `DD/MM/YYYY`. Another records them as `YYYY-MM-DD`. Python can't easily compare the two until they're standardized.\n",
    "\n",
    "3.  **Measurement Error (The Nonsense):**\n",
    "    * **Outliers:** Extreme values that are almost certainly errors.\n",
    "        * *Example:* A survey on income shows one person earning **\\$5,000,000,000** annually. This is likely a data entry error (too many zeros) and, if not corrected, will severely distort the *average* income.\n",
    "    * **Structural Errors:** Columns are mixed up, or a row contains data that clearly belongs in another column.\n",
    "        * *Example:* The 'Student Major' column accidentally has a student's 'GPA' entered in one row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea62268",
   "metadata": {},
   "source": [
    "### **Data Sources: APIs, Web Scraping, Databases, Flat Files, Surveys**\n",
    "\n",
    "What kind of data do you generate on a daily basis? Think about your social media accounts, credit cards, class registration, grades, etc.). Every time you submit a form, post to social media, or make a purchase, that information goes into a **database**.\n",
    "\n",
    "\n",
    "#### **The Spectrum of Data Collection**\n",
    "Data sources can be categorized by how **structured** and how **clean** the data tends to be.\n",
    "\n",
    "| Data Source | Description | Data Structure & Cleanliness |\n",
    "| :--- | :--- | :--- |\n",
    "| **Databases (SQL, NoSQL)** | Highly organized, structured systems (like Excel tables, but massive) used for transactional data (e.g., student grades, financial transactions). | **Highest Quality.** Data is typically clean, structured, and consistent because it's validated when entered. Requires a language like **SQL** to query. |\n",
    "| **Flat Files (CSV, Excel)** | Simple text files where data is delimited (separated) by commas, tabs, or lines. | **Variable Quality (Often Dirty).** Easy to read, but prone to manual errors, inconsistent delimiters, and lack of version control. The most common format for a starter lab. |\n",
    "| **APIs** (Application Programming Interfaces) | A reliable \"front door\" provided by a service (e.g., Google Maps, Twitter) that allows a program to request specific data in a clean format like **JSON**. | **High Quality.** Data is clean and formatted for computer consumption. Requires programming knowledge to write the request and handle the structured response. |\n",
    "| **Surveys (e.g., Google Forms)** | Direct data collected from human input, often involving text responses. | **High Structural Quality, Low Data Quality.** The structure is clean (columns line up), but the *content* is dirty due to human factors (typos, skipped questions, ambiguous answers). |\n",
    "| **Web Scraping** | Using code to extract information from a public website's HTML source code. | **Lowest Quality / Most Unstructured.** Data is messy because it was designed to be *displayed* to a human, not *read* by a machine. Comes with legal/ethical challenges (robots.txt). |\n",
    "\n",
    "#### **API vs. Scraping:**\n",
    "- **API** is the **Front Door:** The service *wants* you to have the data and provides it cleanly.\n",
    "- **Web Scraping** is the **Window:** You're peeking in and trying to grab data that wasn't designed for programmatic access. It's often difficult and messy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c4f61",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "## **The Core Data Structure: Pandas DataFrame**\n",
    "\n",
    "The fundamental and indispensable tool for all stages of data wrangling in the Python ecosystem is the Pandas DataFrame.  It serves as the primary data structure for almost every machine learning, statistical modeling, and visualization task. To grasp its power, think of a DataFrame not just as a supercharged spreadsheet or a table in a relational database, but as a specialized, labeled data container designed specifically for high-performance data manipulation and analysis. The underlying Pandas library is built on top of NumPy, which ensures fast, vector-based operations, making it efficient for handling datasets ranging from thousands to millions of records. This foundation is crucial because it gives analysts a uniform way to represent data, regardless of its original source (CSV, SQL, API), ensuring a standardized workflow for all subsequent operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0322aa",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "## **Web-Based Data Acquisition Tutorial**\n",
    "\n",
    "### **Retrieving Structure Data from APIs with [`requests`](https://docs.python-requests.org/en/latest/)**\n",
    "\n",
    "The `requests` library has become the standard method in Python for making HTTP requests &mdash; the same kind of requests your web browser makes. When dealing with APIs, you use `requests.get()` to query a specific endpoint, which typically returns clean, structured data in **JSON** format.\n",
    "\n",
    "To use `requests`, first make sure you have it installed (via conda): \n",
    "\n",
    "```sh\n",
    "\n",
    "conda install requests\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Here is an example of retrieving data from an API endpoint using `requests`. (We will use a free API specifically designed for testing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2339ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the API endpoint URL\n",
    "API_URL = \"https://jsonplaceholder.typicode.com/users/1\"\n",
    "\n",
    "# Step 2: Make the GET request\n",
    "response = requests.get(API_URL)\n",
    "\n",
    "# Step 3: Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # Step 4: Convert the response JSON text into a Python dictionary\n",
    "    #         and convert the dictionary into a Pandas DataFrame\n",
    "    user_data = response.json()\n",
    "    df_user = pd.DataFrame(user_data)\n",
    "\n",
    "    # Print or load the data\n",
    "    print(\"Successfully fetched user data:\")\n",
    "    print(df_user) # Transpose for better readability\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0eb34",
   "metadata": {},
   "source": [
    "#### **Step 1: Defining the API Endpoint URL**\n",
    "\n",
    "This step is about specifying the exact location of the data you want to fetch on the internet. The API URL (Endpoint) is essentially a unique address that your code will query. It consists of two main parts:\n",
    "\n",
    "1. Base URL (`https://jsonplaceholder.typicode.com`): This is the main address of the server providing the API service. It tells your request where to go.\n",
    "\n",
    "1. Path/Resource (`/users/1`): This part specifies what data you are requesting.\n",
    "\n",
    "    - `/users` indicates you want information about users.\n",
    "\n",
    "    - `/1` acts as a parameter, specifying that you only want the record for the user whose ID is 1.\n",
    "\n",
    "By defining the `API_URL`, you are constructing the equivalent of a web browser link, but one that is optimized to return structured data (JSON) rather than a webpage (HTML).\n",
    "\n",
    "\n",
    "#### **Step 2: Making the GET Request**\n",
    "\n",
    "This step executes the data retrieval process using the defined URL. The `requests.get()` function performs an HTTP GET request. Think of this as your computer sending a literal command to the API server that says, \"Hey, I need the data located at this `API_URL`.\" The result of this function is an object called `response`. This `response` object is critically important because it packages everything the server sends back, including:\n",
    "\n",
    "1. The Content (Data): The actual JSON text containing the user information.\n",
    "\n",
    "1. The Status Code: A three-digit number indicating whether the request was successful (e.g., `200` for OK) or if an error occurred (e.g., `404` for Not Found).\n",
    "\n",
    "1. Headers: Metadata about the response (like the content type, date, and encoding).\n",
    "\n",
    "The remainder of the code (Steps 3, 4, and 5) is dedicated to unpacking and validating the data stored inside this `response` object.\n",
    "\n",
    "\n",
    "#### **Step 3: Checking the Response Status Code**\n",
    "\n",
    "This step is the first and most critical form of error handling. We check the server's immediate verdict on our request. The server always responds with a Status Code, which is a three-digit integer that explains the outcome of the request:\n",
    "\n",
    "- `200` (OK): This is the success code. It confirms that the request was processed, the server found the data, and the data is included in the response body. If we get a `200`, we know we can proceed safely to extract the content.\n",
    "\n",
    "- `4xx` Codes (Client Errors): These indicate you made a mistake (e.g., `404` Not Found, `403` Forbidden because you're missing an API key, or `400` Bad Request).\n",
    "\n",
    "- `5xx` Codes (Server Errors): These indicate the API server itself failed to fulfill the request (e.g., `500` Internal Server Error).\n",
    "\n",
    "By checking `if response.status_code == 200`, we ensure our code only tries to process the data if the request was truly successful, preventing Python exceptions from attempting to parse an error message.\n",
    "\n",
    "\n",
    "#### **Step 4: Converting JSON Text to a Python Dictionary**\n",
    "\n",
    "The data itself arrives as a JSON string—just a long piece of text. This step converts that raw text into Python objects that are easy to access and manipulate. The `response.json()` method is a built-in feature of the `requests` library that does two things:\n",
    "\n",
    "1. Decodes: It reads the JSON text string from the response body.\n",
    "\n",
    "1. Deserializes: It automatically translates the JSON structure into equivalent native Python data structures:\n",
    "\n",
    "    - JSON objects (`{key: value}`) become Python dictionaries.\n",
    "    - JSON arrays (`[item1, item2]`) become Python lists.\n",
    "\n",
    "The result is the `user_data` variable, which is now a highly accessible Python dictionary. We can use standard bracket notation (e.g., `user_data['name']` or `user_data['address']['city']`) to navigate the data.\n",
    "\n",
    "One thing that stands out in the output is that it appears to have repeated entries for some fields. This is because the object returned had nested dictionaries and so things were expanded. To appropriately handle the nested dictionaries, we can leverage the `pd.json_normalize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the API endpoint URL\n",
    "API_URL = \"https://jsonplaceholder.typicode.com/users/1\"\n",
    "\n",
    "# Step 2: Make the GET request\n",
    "response = requests.get(API_URL)\n",
    "\n",
    "# Step 3: Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # Step 4: Convert the response JSON text into a Python dictionary,\n",
    "    #         normalize the data, then convert the dictionary into a Pandas\n",
    "    #         DataFrame\n",
    "    user_data = response.json()\n",
    "    user_data_normalized = pd.json_normalize(user_data)\n",
    "\n",
    "    # Step 5: Convert the nested JSON structure directly into a flattened Pandas DataFrame\n",
    "    df_user = pd.DataFrame(user_data_normalized)\n",
    "\n",
    "    # Print or load the data\n",
    "    print(\"Successfully fetched user data:\")\n",
    "    print(df_user.T) # Transpose for better readability\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bf8ed",
   "metadata": {},
   "source": [
    "#### **Step 5: Flattening Nested Data into a DataFrame**\n",
    "\n",
    "This is where the structure of the data is transformed from a nested hierarchy into a flat, tabular format suitable for analysis. Directly converting nested dictionaries into a standard Pandas DataFrame often results in columns containing complex Python objects (like the whole `address` dictionary living in one cell). The `pd.json_normalize()` function solves this by automatically flattening the nested structure:\n",
    "\n",
    "- Creates Columns: It takes the nested keys and combines them with the parent key using a dot (`.`). For instance, the nested key `city` inside the `address` object becomes the single column name `address.city` in the resulting DataFrame.\n",
    "\n",
    "- Handles Single Records: The API returned a single user, which is a dictionary (`user_data`). `json_normalize` is designed to process a *list of records*, so we wrap the single dictionary in a list (`[user_data]`) to tell Pandas to treat it as a single row in the new DataFrame.\n",
    "\n",
    "The final output, `df_user`, is a clean, single-row DataFrame where every piece of information (like `address.city` and `company.name`) is now its own dedicated column, perfectly ready for data cleaning and analysis.\n",
    "\n",
    "\n",
    "**Retrieving Multiple Records with One Call**\n",
    "\n",
    "Instead of retrieving a single record, you can request all of them by changing the API URL and requests will handle things seemlessly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c4f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the API endpoint URL\n",
    "API_URL = \"https://jsonplaceholder.typicode.com/users/\"\n",
    "\n",
    "# Step 2: Make the GET request\n",
    "response = requests.get(API_URL)\n",
    "\n",
    "# Step 3: Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # Step 4: Convert the response JSON text into a Python dictionary\n",
    "    #         and convert the dictionary into a Pandas DataFrame\n",
    "    user_data = response.json()\n",
    "    df_user = pd.DataFrame(user_data)\n",
    "\n",
    "    # Print or load the data\n",
    "    print(\"Successfully fetched user data:\")\n",
    "    print(df_user)\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43957ae",
   "metadata": {},
   "source": [
    "And we can use `pd.json_normalize()` on this new requests object as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the API endpoint URL\n",
    "API_URL = \"https://jsonplaceholder.typicode.com/users/\"\n",
    "\n",
    "# Step 2: Make the GET request\n",
    "response = requests.get(API_URL)\n",
    "\n",
    "# Step 3: Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # Step 4: Convert the response JSON text into a Python dictionary,\n",
    "    #         normalize the data, then convert the dictionary into a Pandas\n",
    "    #         DataFrame\n",
    "    user_data = response.json()\n",
    "    user_data_normalized = pd.json_normalize(user_data)\n",
    "\n",
    "    # Step 5: Convert the nested JSON structure directly into a flattened Pandas DataFrame\n",
    "    df_user = pd.DataFrame(user_data_normalized)\n",
    "\n",
    "    # Print or load the data\n",
    "    print(\"Successfully fetched user data:\")\n",
    "    print(df_user)\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989b332",
   "metadata": {},
   "source": [
    "Requests will also allow you to submit parameters as a query to an API. This is handled by passing the `requests.get()` function a `params` keyword argument. `params` should be a dictionary with the `key:pair` being the parameter:value you would include in a URL (the stuff after a `?` in a URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define the API endpoint URL (Base URL for Users)\n",
    "# We are now targeting the '/users' resource, not posts.\n",
    "API_URL = \"https://jsonplaceholder.typicode.com/users\"\n",
    "\n",
    "# 2. Define the Query Parameters (The filter criteria)\n",
    "# This dictionary will be converted to: ?id=1\n",
    "# Note: For the users endpoint, 'id' filters the list of users.\n",
    "params = {\n",
    "    \"id\": 1\n",
    "}\n",
    "\n",
    "# 3. Make the GET request, passing the parameters\n",
    "# requests automatically appends the params to the URL:\n",
    "# \"https://jsonplaceholder.typicode.com/users?id=1\"\n",
    "response = requests.get(API_URL, params=params)\n",
    "\n",
    "# 4. Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # 5. Convert the response JSON text into a Python list of dictionaries\n",
    "    # This API endpoint returns a list of user objects\n",
    "    list_of_users = response.json()\n",
    "\n",
    "    # 6. Convert the list of dictionaries directly into a Pandas DataFrame\n",
    "    # Since the JSON contains nested fields (address, company), json_normalize is ideal\n",
    "    df_users = pd.json_normalize(list_of_users)\n",
    "\n",
    "    # Print the resulting DataFrame summary\n",
    "    print(f\"Successfully fetched {len(df_users)} user record(s) matching ID 1.\")\n",
    "    print(\"\\nDataFrame structure (flattened, transposed for readability):\")\n",
    "\n",
    "    # Display the transposed output to show all flattened columns\n",
    "    if not df_users.empty:\n",
    "        print(df_users.T)\n",
    "    else:\n",
    "        print(\"No user records found.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81084203",
   "metadata": {},
   "source": [
    "You can even put Python objects into the dictionary and requests will automatically expand them correctly for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faccb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define the API endpoint URL (Base URL for Users)\n",
    "# We are now targeting the '/users' resource, not posts.\n",
    "API_URL = \"https://jsonplaceholder.typicode.com/users\"\n",
    "\n",
    "# 2. Define the Query Parameters (The filter criteria)\n",
    "# This dictionary will be converted to: ?id=1\n",
    "# Note: For the users endpoint, 'id' filters the list of users.\n",
    "params = {\n",
    "    \"id\": range(5)\n",
    "}\n",
    "\n",
    "# 3. Make the GET request, passing the parameters\n",
    "# requests automatically appends the params to the URL:\n",
    "# \"https://jsonplaceholder.typicode.com/users?id=1\"\n",
    "response = requests.get(API_URL, params=params)\n",
    "\n",
    "# 4. Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # 5. Convert the response JSON text into a Python list of dictionaries\n",
    "    # This API endpoint returns a list of user objects\n",
    "    list_of_users = response.json()\n",
    "\n",
    "    # 6. Convert the list of dictionaries directly into a Pandas DataFrame\n",
    "    # Since the JSON contains nested fields (address, company), json_normalize is ideal\n",
    "    df_users = pd.json_normalize(list_of_users)\n",
    "\n",
    "    # Print the resulting DataFrame summary\n",
    "    print(f\"Successfully fetched {len(df_users)} user record(s) matching ID 1.\")\n",
    "    print(\"\\nDataFrame structure (flattened, transposed for readability):\")\n",
    "\n",
    "    # Display the transposed output to show all flattened columns\n",
    "    if not df_users.empty:\n",
    "        print(df_users.T)\n",
    "    else:\n",
    "        print(\"No user records found.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b507d",
   "metadata": {},
   "source": [
    "Here is an example that has two separate filters: one using a python object and one using a specific value within the username field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c5ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define the API endpoint URL (Base URL for Users)\n",
    "# We are now targeting the '/users' resource.\n",
    "API_URL = \"https://jsonplaceholder.typicode.com/users\"\n",
    "\n",
    "# 2. Define the Query Parameters (The filter criteria)\n",
    "# We are now requesting MULTIPLE distinct parameters:\n",
    "# - Filter by ID 1\n",
    "# - AND Filter by username 'Bret'\n",
    "params = {\n",
    "    \"id\": range(5),\n",
    "    \"username\": \"Bret\"\n",
    "}\n",
    "\n",
    "# 3. Make the GET request, passing the parameters\n",
    "# requests automatically appends both parameters to the URL:\n",
    "# \"https://jsonplaceholder.typicode.com/users?id=1&username=Bret\"\n",
    "response = requests.get(API_URL, params=params)\n",
    "\n",
    "# 4. Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # 5. Convert the response JSON text into a Python list of dictionaries\n",
    "    list_of_users = response.json()\n",
    "\n",
    "    # 6. Convert the list of dictionaries directly into a Pandas DataFrame\n",
    "    df_users = pd.json_normalize(list_of_users)\n",
    "\n",
    "    # Print the resulting DataFrame summary\n",
    "    print(f\"Successfully fetched {len(df_users)} user record(s) matching ID 1 AND username 'Bret'.\")\n",
    "    print(\"\\nDataFrame structure (flattened, result shown):\")\n",
    "\n",
    "    # Display the transposed output to show all flattened columns\n",
    "    if not df_users.empty:\n",
    "        # Show one row, as the combined filters should result in a single user\n",
    "        print(df_users.head(1).T.to_string())\n",
    "    else:\n",
    "        print(\"No user records found matching both criteria.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a7429",
   "metadata": {},
   "source": [
    "### **Retrieving Unstructured Data with Web Scraping ([`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/))**\n",
    "\n",
    "Web scraping is necessary when a website does not provide an API and you need data that is embedded directly in the HTML designed for human viewing. The process involves two key steps:\n",
    "\n",
    "1. **Fetch the HTML:** Use the `requests` module again to download the raw HTML content of the page.\n",
    "\n",
    "1. **Parse the HTML:** Use the `BeautifulSoup` library to navigate and extract data from the messy HTML structure.\n",
    "\n",
    "First, ensure you have both libraries installed: \n",
    "\n",
    "```sh\n",
    "\n",
    "conda install beautifulsoup4\n",
    "\n",
    "```\n",
    "\n",
    "#### Example: Scraping a Website Title and Paragraph\n",
    "\n",
    "This example simulates scraping the header and a list of books for sale from a web page. (We are using a specific website designed for web scraping testing, so the data may be \"cleaner\" than normal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ac3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the target URL (using a simple placeholder page)\n",
    "URL = \"http://books.toscrape.com/index.html\" # A test site designed for scraping\n",
    "\n",
    "# Step 2: Make the GET request to get the HTML content\n",
    "response = requests.get(URL)\n",
    "response.raise_for_status() # Raise an exception for bad status codes\n",
    "\n",
    "# Step 3: Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 4: Use CSS selectors to find specific elements (The Title)\n",
    "# We look for the h1 element\n",
    "page_title = soup.find('h1').text.strip()\n",
    "\n",
    "# Step 5: Find all elements of a specific class (The Book Titles)\n",
    "# We look for the <h3> tag inside an article element, specifically the title text\n",
    "book_titles = [\n",
    "    h3.a['title'] # Get the 'title' attribute of the <a> tag inside the <h3>\n",
    "    for h3 in soup.find_all('h3')\n",
    "]\n",
    "\n",
    "# Print the extracted data\n",
    "print(f\"Scraped Page Title: {page_title}\")\n",
    "print(\"\\nFirst 5 Book Titles Scraped:\")\n",
    "for title in book_titles[:5]:\n",
    "    print(f\"- {title}\")\n",
    "\n",
    "# Data can be easily converted to a DataFrame for cleaning/analysis\n",
    "df_books = pd.DataFrame({'Title': book_titles})\n",
    "print(\"\\nDataFrame of Book Titles:\")\n",
    "print(df_books.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785e161",
   "metadata": {},
   "source": [
    "#### **Step 1: Defining the Target URL**\n",
    "\n",
    "Similar to the API request, this step identifies the exact source of the data.\n",
    "\n",
    "- **Purpose:** The `URL` here points to a standard web page designed to be rendered by a browser for a human reader. The server responds with a giant block of **HTML text** (which contains all the page content, layout instructions, links, etc.) rather than a neat, structured JSON object.\n",
    "\n",
    "- **Context:** Unlike an API, where the URL parameters filter the data, this URL is the address of the entire document we intend to consume whole.\n",
    "\n",
    "\n",
    "#### **Step 2: Making the GET Requests and Checking Status**\n",
    "\n",
    "This step uses the same `requests` library to fetch the raw data, but it handles error checking slightly differently.\n",
    "\n",
    "- **`requests.get(URL)`**: Fetches the entire HTML document from the server and stores it in the response object. The actual HTML content is held in the property `response.text`.\n",
    "\n",
    "- **`response.raise_for_status()`**: This is a streamlined way to handle status codes. If the code is not in the successful `2xx` range (e.g., if it's a `404` Not Found or `500` Server Error), this method will immediately raise an HTTPE**rror exception**. This prevents the rest of your script from running on bad or missing data, which is crucial for robust scraping.\n",
    "\n",
    "\n",
    "#### **Step 3: Creating the BeautifulSoup Object (Parsing)**\n",
    "\n",
    "This is the central step that transforms the raw HTML string into a structured, searchable object.\n",
    "\n",
    "- **Raw Input**: `response.text` is the raw, unformatted HTML text string (e.g., `<html><head>...</head><body><h1>...`).\n",
    "\n",
    "- **The Parser `(html.parser)`**: This argument tells BeautifulSoup how to interpret the raw string. The parser organizes the string according to the rules of HTML structure, creating an internal **Document Object Model (DOM)** &ndash; a navigable, tree-like structure where elements like `<h1>`, `<a>`, and `<div>` are nodes you can easily travel between.\n",
    "\n",
    "- **The Output (`soup`)**: The `soup` object is the \"prepared\" version of the website. It allows you to use methods like `.find()` and `.find_all()` to search the document structure rather than searching plain text.\n",
    "\n",
    "\n",
    "#### **Step 4: Finding Specific Elements (Single Match)**\n",
    "\n",
    "This step demonstrates the simplest way to locate a unique piece of data.\n",
    "\n",
    "- **`.find('h1')`**: This method searches the entire `soup` structure (the DOM) for the first occurrence of the specified HTML tag (`<h1>`). It returns a single **Tag object** representing that element.\n",
    "\n",
    "- **`.text`**: Once the tag is found, this property extracts only the visible text content **inside** the tag, discarding the HTML markup itself (e.g., turning `<h1>Book Titles</h1>` into just `Book Titles`).\n",
    "\n",
    "- **`.strip()`**: A common cleanup function in Python that removes any unnecessary whitespace (spaces, newlines, tabs) from the beginning and end of the extracted text.\n",
    "\n",
    "\n",
    "#### **Step 5: Finding Multiple Elements (List Extraction)**\n",
    "\n",
    "This step shows how to extract a collection of data points, often by targeting a common class or structure, using list comprehension for efficiency.\n",
    "\n",
    "- **`.find_all('h3')`**: This searches the entire document and returns a list of all Tag objects that match the specified tag (`<h3>`).\n",
    "\n",
    "- **List Comprehension**: The syntax `[expression for item in list]` is a Python shortcut for iterating over the list of `<h3>` elements.\n",
    "\n",
    "- **Deep Navigation (`h3.a['title']`)**: This is where the power of BeautifulSoup shines. For each `<h3>` element found:\n",
    "\n",
    "    -`.a`: We look inside the `<h3>` tag to find the nested link tag (`<a>`).\n",
    "\n",
    "    - `['title']`: We then don't extract the text, but rather the value of the `title` **attribute** within that link tag, which is where the full book title is stored in this specific website's HTML structure.\n",
    "\n",
    "This final process converts a list of complex HTML objects into a clean, simple Python list of strings (bo  `ok_titles`), which is then ready to be loaded into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c176bcc",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "## **Phase I: Cleaning the Data (The Essential 80%)**\n",
    "\n",
    "Data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset. It ensures data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fafda3",
   "metadata": {},
   "source": [
    "### **Handling Missing Data (Should [Mostly] Be a Review of Week03!)**\n",
    "\n",
    "Missing values, often represented as `NaN` (Not a Number) or `null`, are one of the most common problems.\n",
    "\n",
    "- **Removing Observations with Missing Values:**\n",
    "\n",
    "    - **Concept:** Deleting entire rows (observations) or columns (variables) that contain missing values.\n",
    "\n",
    "    - **Python Method:** `df.dropna(axis=0)` (removes rows), `df.dropna(axis=1)` (removes columns).\n",
    "\n",
    "    - **When to Use:** When the number of missing values is very small compared to the total dataset, or when the missingness is *random* and dropping the data won't introduce bias.\n",
    "\n",
    "- **Imputation from Internal Data (Central Tendency):**\n",
    "\n",
    "    - **Concept:** Replacing missing values with a calculated value from the same variable.\n",
    "\n",
    "    - **Techniques:**\n",
    "\n",
    "        - **Mean:** Use the average for quantitative data. (Sensitive to outliers).\n",
    "\n",
    "        - **Median:** Use the middle value for quantitative data. (Robust to outliers).\n",
    "\n",
    "        - **Mode:** Use the most frequent value for categorical data.\n",
    "\n",
    "    - **Python Method:** `df['column'].fillna(df['column'].mean(), inplace=True)`.\n",
    "\n",
    "    - **When to Use:** When the distribution of the data is stable and the assumption that the missing value is \"average\" is reasonable.\n",
    "\n",
    "- **Imputation from External Data (Advanced):**\n",
    "\n",
    "    - **Concept:** Using information outside the column (e.g., from other columns or external sources) to predict the missing value.\n",
    "\n",
    "    - **Techniques:**\n",
    "\n",
    "        - **Conditional Imputation:** Impute the mean/median based on a *group* (e.g., fill in missing 'Salary' based on the median 'Salary' for their 'Job Title').\n",
    "\n",
    "        - **Model-Based Imputation:** Use a machine learning model (like linear regression) to predict the missing value based on other features.\n",
    "\n",
    "    - **When to Use:** When missingness is substantial or clearly related to another variable.\n",
    "\n",
    "- **Other Options for Missing Data:**\n",
    "\n",
    "    - **Flagging:** Create a new binary variable (e.g., `is_salary_missing`) and impute a placeholder value (like -1). This lets the model know the value was originally missing.\n",
    "\n",
    "    - **Temporal/Sequential Imputation:** Use the previous or next value (e.g., for time series data using `ffill` or `bfill`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad78023",
   "metadata": {},
   "source": [
    "### **Validating Data Content**\n",
    "\n",
    "We must ensure the values that *are* present make logical sense.\n",
    "\n",
    "- **Implausible Values (Logical Errors):**\n",
    "\n",
    "    - **Concept:** Values that are technically numbers or text but are logically impossible.\n",
    "    \n",
    "    - **Example:** Age of 200 years, a height of 0 inches, a product price of $-5.\n",
    "\n",
    "    - **Action:** Filter them out using boolean indexing and treat them as missing data (e.g., replace with `NaN`) for later imputation.\n",
    "\n",
    "    - **Python Method:** `df.loc[df['Age'] > 120, 'Age'] = np.nan`\n",
    "\n",
    "- **Extreme Data Values (Outliers):**\n",
    "\n",
    "    - **Concept:** Values that deviate significantly from the rest of the distribution, often due to error or rare events.\n",
    "\n",
    "    - **Detection:** Use statistical methods like the **Z-score** ($Z = (x - \\mu) / \\sigma$) or the **Interquartile Range (IQR)**\n",
    "\n",
    "    - **Action:** Depending on the context, you can remove them (if they are clearly errors) or cap/winsorize them (replace them with the nearest non-outlier value) if they represent true, rare events. (Winsorize is a statistical method to reduce the influence of outliers by replacing extreme data points with less extreme values, rather than removing them.)\n",
    "\n",
    "    ![Box Plot](week05-lecture-files/boxplot-from-google.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e7aa1d",
   "metadata": {},
   "source": [
    "### **Correcting Data Structure and Identity**\n",
    "\n",
    "Data must be uniformly structured and unique.\n",
    "\n",
    "- **Incorrect Formats (Standardization):**\n",
    "\n",
    "    - **Concept:** Ensuring all entries in a column follow the same standard.\n",
    "\n",
    "    - **Examples:**\n",
    "\n",
    "        - **Text:** Lowercasing/uppercasing all text (e.g., making all city names uniform: `'dallas'` vs. `'Dallas'`).\n",
    "\n",
    "        - **Date:** Converting all date formats to a single standard (e.g., `YYYY-MM-DD`).\n",
    "\n",
    "    - **Python Method:** `.str.lower()`, `pd.to_datetime()`.\n",
    "\n",
    "- **Duplicate Records (Deduplication):**\n",
    "\n",
    "    - **Concept:** Identifying and removing rows that are identical or near-identical.\n",
    "\n",
    "    - **Python Method:** `df.drop_duplicates()`.\n",
    "\n",
    "    - **Parameters:** Use the `subset` parameter to check for duplicates based on only a few key columns (e.g., checking for duplicate customers based on 'Name' and 'Email').\n",
    "\n",
    "    - **Python Demo Code:** See the functions in `Data Cleaning Demo` cell at the end of this section of the notebook for all these techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83a780",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "## **Phase 2: Transforming the Data (Feature Engineering)**\n",
    "\n",
    "Data transformation, or Feature Engineering, means changing the scale, shape, or meaning of variables to make them more useful for modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf663b31",
   "metadata": {},
   "source": [
    "### **Manipulating Quantitative Variables**\n",
    "\n",
    "Quantitative (numerical) variables can be modified for better model performance.\n",
    "\n",
    "- **Creating a Categorical Variable from a Quantitative Variable (Binning):**\n",
    "\n",
    "    - **Concept:** Grouping continuous values into discrete categories (bins).\n",
    "\n",
    "    - **Example:** Turning 'Age' (0-100) into 'Age Group' (Child, Teen, Adult, Senior).\n",
    "\n",
    "    - **Python Method:** `pd.cut()` or `pd.qcut()`.\n",
    "\n",
    "- **Changing the Scale of a Quantitative Variable (Scaling/Normalization):**\n",
    "\n",
    "    - **Concept:** Rescaling the feature values to a fixed range (usually 0 to 1) or a standard distribution (mean 0, std dev 1). This is vital for many distance-based algorithms (k-NN, clustering).\n",
    "\n",
    "    - **Min-Max Scaling:** Rescales data to the range $[0, 1]$.\n",
    "    \n",
    "    $$ x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n",
    "\n",
    "    - **Standardization (Z-Score):** Rescales data to have a mean of 0 and standard deviation of 1.\n",
    "    \n",
    "    $$x_{standardized} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "  - **Python Method:** Often done using scikit-learn's `MinMaxScaler` or `StandardScaler`.\n",
    "\n",
    "- **Combining Two or More Quantitative Variables (Feature Creation):**\n",
    "\n",
    "    - **Concept:** Creating a new, more meaningful variable by combining existing ones using mathematical operations.\n",
    "\n",
    "    - **Example:** Creating **BMI** from Height and Weight ($BMI = Weight / Height^2$).\n",
    "\n",
    "    - **Python Method:** Simple arithmetic operations on DataFrame columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908d5f4",
   "metadata": {},
   "source": [
    "### **Manipulating Categorical Variables**\n",
    "\n",
    "Categorical variables (e.g., colors, regions) often need to be prepared for mathematical models.\n",
    "\n",
    "- **Creating a Quantitative Variable from a Categorical Variable (One-Hot Encoding):**\n",
    "\n",
    "    - **Concept:** Converting categories into a series of binary (0 or 1) columns. This allows models to use them mathematically.\n",
    "\n",
    "    - **Example:** The 'Color' column with values `['Red', 'Blue', 'Red']` is converted into two new columns: `'Color_Red'` and `'Color_Blue'`.\n",
    "\n",
    "    - **Python Method:** `pd.get_dummies()`.\n",
    "\n",
    "- **Condensing the Categories of a Categorical Variable:**\n",
    "\n",
    "    - **Concept:** Reducing the number of unique categories when many have very few observations (low-frequency categories).\n",
    "\n",
    "    - **Example:** Combining all job titles with $< 50$ entries into a single category called 'Other'.\n",
    "\n",
    "    - **Python Method:** Use `.value_counts()` to identify low-frequency categories, then use the `.replace()` method to group them.\n",
    "\n",
    "- **Splitting a Variable into Multiple Variables:**\n",
    "\n",
    "    - **Concept:** Extracting components from a single string variable into multiple distinct columns.\n",
    "\n",
    "    - **Example:** Splitting the address column `'123 Main St, Dallas, TX'` into separate 'Street', 'City', and 'State' columns.\n",
    "\n",
    "    - **Python Method:** `.str.split()` and `.str.extract()`.\n",
    "\n",
    "    - **Python Demo Code:** See the functions in `Data Transformation Demo` cell at the end of this section of the notebook for all these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef39783",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "## **Phase 3: Integrating the Data (Combining Sources)**\n",
    "\n",
    "Integration involves combining data from multiple sources (DataFrames) to create a single, comprehensive view."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800bbd99",
   "metadata": {},
   "source": [
    "### **Stacking Datasets (Concatenation)**\n",
    "\n",
    "- **Concept:** Appending one dataset below another. Useful when datasets have the same columns but different rows (e.g., Sales data from January and Sales data from February).\n",
    "\n",
    "- **Python Method:** `pd.concat([df1, df2])`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fa29c",
   "metadata": {},
   "source": [
    "### **Augmenting/Merging Datasets (Joining)**\n",
    "\n",
    "- **Concept:** Combining two datasets side-by-side based on a common key (a unique identifier, like 'Customer ID' or 'Product SKU'). This is similar to a SQL JOIN.\n",
    "\n",
    "- **Merge Types:**\n",
    "\n",
    "    - **Inner Join:** Keeps only rows where the key exists in *both* DataFrames.\n",
    "\n",
    "    - **Left Join:** Keeps all rows from the *left* DataFrame and matches from the right.\n",
    "\n",
    "    - **Right Join:** Keeps all rows from the *right* DataFrame and matches from the left.\n",
    "\n",
    "    - **Outer Join:** Keeps all rows from *either* DataFrame (fills missing matches with `NaN`).\n",
    "\n",
    "    - **Python Method:** `pd.merge(df_left, df_right, on='key_column', how='inner')`.\n",
    "\n",
    "    - **Python Demo Code:** See the functions in `Data Integration Demo` cell at the end of this section of the notebook for all these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0399faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Demo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Setup: Create a simulated DataFrame with cleaning issues ---\n",
    "data = {\n",
    "    'ID': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "    'Age': [25, 30, np.nan, 45, 150, 32, 28, 90, 40, 35], # Missing & Implausible\n",
    "    'Salary': [50000, 60000, 75000, 55000, 1000000, 60000, 80000, 95000, 75000, np.nan], # Outlier & Duplicate & Missing\n",
    "    'City': ['new york ', 'London', 'paris', 'New York', 'London', 'Paris', 'New York', 'London', 'paris', 'Paris'], # Formatting/Duplicates\n",
    "    'Is_Active': [True, True, True, False, True, True, True, False, True, True],\n",
    "    'Notes': ['N/A', '', 'Error', 'N/A', 'ok', '', 'Error', 'ok', 'N/A', 'ok']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"--- Initial Dirty DataFrame ---\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 2. Missing Data Handling ---\n",
    "\n",
    "# Identify missing values\n",
    "print(\"Missing values before cleaning:\\n\", df.isnull().sum())\n",
    "\n",
    "# A. Removing Observations (dropna)\n",
    "# Remove the row where Age is missing (ID 103)\n",
    "df_drop = df.dropna(subset=['Age'])\n",
    "print(\"\\nA. After dropping rows with missing 'Age' (ID 103 removed):\\n\", df_drop)\n",
    "\n",
    "# B. Imputation from Internal Data (Median for Salary)\n",
    "# Calculate the median of the non-outlier Salary (We'll fix the outlier in the next step first!)\n",
    "# We will use the original df for imputation to demonstrate the technique clearly.\n",
    "median_salary = df['Salary'].median() # Approx 75000.0\n",
    "df.fillna({'Salary': median_salary}, inplace=True)\n",
    "print(\"\\nB. Salary imputed with Median (ID 110 updated):\\n\", df)\n",
    "\n",
    "# --- 3. Implausible and Extreme Data Values ---\n",
    "\n",
    "# A. Implausible Values (Age > 120)\n",
    "# Replace implausible ages (e.g., 150) with NaN for later handling\n",
    "df.loc[df['Age'] > 120, 'Age'] = np.nan\n",
    "print(\"\\nA. Replaced implausible Age (150 for ID 105) with NaN:\\n\", df)\n",
    "\n",
    "# B. Handling Extreme Values (Outliers - Capping Salary)\n",
    "# We see an outlier Salary of 1,000,000 (ID 105). Let's cap it at the 95th percentile.\n",
    "upper_bound = df['Salary'].quantile(0.95) # Calculate the 95th percentile\n",
    "# We will use a safe upper bound value for this small, noisy dataset\n",
    "safe_cap = 100000\n",
    "df.loc[df['Salary'] > safe_cap, 'Salary'] = safe_cap\n",
    "print(f\"\\nB. Salary capped (Outlier 1,000,000 for ID 105 capped to {safe_cap}):\\n\", df)\n",
    "\n",
    "# C. Imputation (Post-implausible fix)\n",
    "# Impute the newly created NaN in Age (ID 105) with the median age.\n",
    "median_age = df['Age'].median() # Approx 33.5\n",
    "df.fillna({\"Age\": median_age}, inplace=True)\n",
    "print(f\"\\nC. Age imputed with Median (ID 105 updated):\\n\", df)\n",
    "\n",
    "\n",
    "# --- 4. Incorrect Formats and Duplicate Records ---\n",
    "\n",
    "# A. Incorrect Formats (Text Standardization and Whitespace)\n",
    "# Standardize 'City' to lowercase and strip whitespace\n",
    "df['City'] = df['City'].str.lower().str.strip('\\n')\n",
    "print(\"\\nA. Standardized City format (lowercase and stripped whitespace):\\n\", df['City'].value_counts())\n",
    "\n",
    "# B. Duplicate Records (Removing Exact Duplicates)\n",
    "# Notice ID 103 and 109 both have Salary 75000 and City 'paris'\n",
    "# After cleaning, we check for row-level duplicates (none are exact here)\n",
    "# Let's manually create an exact duplicate for demonstration:\n",
    "df.loc[10, :] = df.loc[9, :] # Create a duplicate of row 10 (index 9)\n",
    "df.loc[10, 'ID'] = 111 # Give it a new (but functionally duplicate) ID\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "print(\"\\nBefore drop_duplicates (11 rows):\\n\", df)\n",
    "# Drop duplicates based on 'Age', 'Salary', and 'City' (excluding 'ID' and 'Is_Active')\n",
    "df_deduped = df.drop_duplicates(subset=['Age', 'Salary', 'City'], keep='first')\n",
    "print(\"\\nB. After dropping duplicates based on key columns (Row 11 removed):\\n\", df_deduped)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566cb7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation Demo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# --- 1. Setup: Create a clean, simulated DataFrame for transformation ---\n",
    "data = {\n",
    "    'Student_ID': [101, 102, 103, 104, 105, 106, 107, 108],\n",
    "    'GPA': [3.5, 2.1, 4.0, 3.9, 2.5, 3.2, 1.9, 3.8],\n",
    "    'Study_Hours': [15, 5, 25, 22, 10, 18, 3, 20],\n",
    "    'Exam_Score': [85, 62, 98, 95, 70, 88, 55, 92],\n",
    "    'Address': ['123 Main St; NYC, NY', '45 North Ave; New Orleans, LA', '78 Pine Ln; Miami, FL', '22 Oak Ct; NYC, NY', '55 Elm Blvd; Miami, FL', '99 Hill Rd; New Orleans, LA', '11 Bay Pk; Miami, FL', '33 Vine Way; New Orleans, LA'],\n",
    "    'Major': ['CS', 'History', 'CS', 'Math', 'History', 'Math', 'History', 'CS'],\n",
    "    'Enrollment_Status': ['Full-Time', 'Part-Time', 'Full-Time', 'Full-Time', 'Part-Time', 'Full-Time', 'Part-Time', 'Full-Time']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"--- Initial Clean DataFrame for Transformation ---\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 2. Manipulating Quantitative Variables ---\n",
    "\n",
    "# A. Creating a Categorical Variable from a Quantitative Variable (Binning GPA)\n",
    "# Define bins for GPA (e.g., Low, Medium, High)\n",
    "bins = [0, 2.5, 3.5, 4.0]\n",
    "labels = ['Below Avg (0-2.5)', 'Avg (2.5-3.5)', 'Above Avg (3.5-4.0)']\n",
    "df['GPA_Group'] = pd.cut(df['GPA'], bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "print(\"A. GPA binned into Categorical Variable (GPA_Group):\\n\", df[['GPA', 'GPA_Group']])\n",
    "\n",
    "# B. Changing the Scale of a Quantitative Variable (Min-Max Scaling Study_Hours)\n",
    "# Initialize the Scaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "# Fit and transform the 'Study_Hours' column\n",
    "df['Hours_Scaled'] = min_max_scaler.fit_transform(df[['Study_Hours']])\n",
    "print(\"\\nB. Study_Hours scaled to [0, 1] (Min-Max Scaling):\\n\", df[['Study_Hours', 'Hours_Scaled']])\n",
    "\n",
    "# C. Combining Two or More Quantitative Variables (Creating a 'Performance Index')\n",
    "# Simple index: (GPA * 10) + Exam_Score - Study_Hours\n",
    "df['Performance_Index'] = (df['GPA'] * 10) + df['Exam_Score'] - df['Study_Hours']\n",
    "print(\"\\nC. New Quantitative Feature (Performance_Index) created:\\n\", df[['GPA', 'Exam_Score', 'Study_Hours', 'Performance_Index']])\n",
    "\n",
    "# --- 3. Manipulating Categorical Variables ---\n",
    "\n",
    "# A. Creating a Quantitative Variable from a Categorical Variable (One-Hot Encoding Major)\n",
    "df_encoded = pd.get_dummies(df, columns=['Major'], prefix='Major')\n",
    "print(\"\\nA. Categorical Variable 'Major' One-Hot Encoded:\\n\", df_encoded[['Student_ID', 'Major_CS', 'Major_History', 'Major_Math']])\n",
    "\n",
    "# B. Condensing the Categories of a Categorical Variable (Low-Frequency Grouping)\n",
    "# For this small dataset, we'll pretend 'Part-Time' is low frequency and group it into 'Other'\n",
    "status_counts = df['Enrollment_Status'].value_counts()\n",
    "low_freq_categories = status_counts[status_counts < 4].index # Finds Part-Time and others if present\n",
    "df['Status_Condensed'] = df['Enrollment_Status'].replace(low_freq_categories, 'Other')\n",
    "print(\"\\nB. Enrollment Status Condensed (Part-Time grouped):\\n\", df['Status_Condensed'].value_counts())\n",
    "\n",
    "# C. Splitting a Variable into Multiple Variables (Splitting Address)\n",
    "# Extract the City from the Address string (assuming City is before the comma)\n",
    "df[['Street', 'City_State']] = df['Address'].str.split('; ', expand=True)\n",
    "df[['City', 'State']] = df['City_State'].str.split(', ', expand=True)\n",
    "df.drop(columns=['City_State'], inplace=True) # Drop the temporary column\n",
    "print(\"\\nC. Address Split into 'Street', 'City', 'State' Columns:\\n\", df[['Address', 'Street', 'City', 'State']])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Integration Demo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Setup: Create simulated DataFrames for integration ---\n",
    "\n",
    "# DataFrame 1: Q1 Sales Data (Same columns, different rows)\n",
    "df_q1 = pd.DataFrame({\n",
    "    'Product_ID': [1, 2, 3, 4],\n",
    "    'Region': ['East', 'West', 'East', 'Central'],\n",
    "    'Sales_Q1': [1000, 1500, 1200, 800]\n",
    "})\n",
    "\n",
    "# DataFrame 2: Q2 Sales Data (Same columns, different rows)\n",
    "df_q2 = pd.DataFrame({\n",
    "    'Product_ID': [3, 4, 5, 6],\n",
    "    'Region': ['East', 'Central', 'West', 'South'],\n",
    "    'Sales_Q2': [1300, 900, 1100, 700]\n",
    "})\n",
    "\n",
    "# DataFrame 3: Product Metadata (Different columns, common key 'Product_ID')\n",
    "df_meta = pd.DataFrame({\n",
    "    'Product_ID': [1, 2, 3, 4, 7], # Note Product 7 is metadata only\n",
    "    'Category': ['Electronics', 'Clothing', 'Electronics', 'Home Goods', 'Auto'],\n",
    "    'Cost': [400, 50, 450, 300, 1000]\n",
    "})\n",
    "\n",
    "print(\"--- DataFrames for Integration ---\")\n",
    "print(\"DF Q1 Sales:\\n\", df_q1)\n",
    "print(\"\\nDF Q2 Sales:\\n\", df_q2)\n",
    "print(\"\\nDF Metadata:\\n\", df_meta)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 2. Stacking Datasets (Concatenation) ---\n",
    "\n",
    "# A. Stacking Q1 and Q2 sales (Combining datasets with same columns)\n",
    "# Note: Since Q1 has Sales_Q1 and Q2 has Sales_Q2, let's rename the columns to be compatible.\n",
    "df_q1_r = df_q1.rename(columns={'Sales_Q1': 'Sales'})\n",
    "df_q2_r = df_q2.rename(columns={'Sales_Q2': 'Sales'})\n",
    "\n",
    "# Add a Quarter column for clarity before stacking\n",
    "df_q1_r['Quarter'] = 'Q1'\n",
    "df_q2_r['Quarter'] = 'Q2'\n",
    "\n",
    "df_stacked = pd.concat([df_q1_r, df_q2_r], ignore_index=True)\n",
    "print(\"A. Stacked Dataset (Q1 and Q2 Sales concatenated):\\n\", df_stacked)\n",
    "\n",
    "# --- 3. Augmenting/Merging Datasets (Joins) ---\n",
    "\n",
    "# We will merge the stacked sales data (df_stacked) with the metadata (df_meta) using 'Product_ID'\n",
    "\n",
    "# B. Augmenting/Merging (Inner Join)\n",
    "# Keeps only records where Product_ID is present in BOTH dataframes (1, 2, 3, 4)\n",
    "df_inner = pd.merge(df_stacked, df_meta, on='Product_ID', how='inner')\n",
    "print(\"\\nB. Inner Join (Only common Product_IDs):\\n\", df_inner)\n",
    "\n",
    "# C. Augmenting/Merging (Left Join)\n",
    "# Keeps all records from the LEFT (df_stacked), filling missing metadata with NaN (for Product 5, 6)\n",
    "df_left = pd.merge(df_stacked, df_meta, on='Product_ID', how='left')\n",
    "print(\"\\nC. Left Join (All rows from Sales, NaN for missing metadata):\\n\", df_left)\n",
    "\n",
    "# D. Augmenting/Merging (Outer Join)\n",
    "# Keeps all records from EITHER dataframe (includes Sales data from Q1/Q2 AND Product 7 metadata)\n",
    "df_outer = pd.merge(df_stacked, df_meta, on='Product_ID', how='outer')\n",
    "print(\"\\nD. Outer Join (All possible rows, including Product 7 and Q2 data):\\n\", df_outer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5e670",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "## **Glossary: Core Data Wrangling Definitions**\n",
    "\n",
    "Here are some core definitions used in data science, especially in the context of Python and the pandas library.\n",
    "\n",
    "* **Data Wrangling**: The entire process of cleaning, structuring, and enriching raw, messy data into a more usable format for analysis. It's an umbrella term that includes data preprocessing and transformation.\n",
    "\n",
    "* **Data Preprocessing**: The steps taken to prepare raw data for a machine learning model. This often involves cleaning data (handling missing values, duplicates), scaling numerical features, and encoding categorical variables.\n",
    "\n",
    "* **Data Transformation**: The process of converting data from one format or structure to another. Examples include normalizing numerical data, converting data types, and creating new features from existing ones.\n",
    "\n",
    "* **Dataframe**: A two-dimensional, size-mutable, and potentially heterogeneous tabular data structure with labeled axes (rows and columns). It is the most widely used data structure in the pandas library. \n",
    "\n",
    "* **Imputation**: The technique of filling in or replacing missing data points (`NaN` values) with substitute values. Common imputation strategies include using the mean, median, or a constant value.\n",
    "\n",
    "* **Mean**: The average of a set of numerical values. It is calculated by summing all the values and dividing by the count of the values.\n",
    "\n",
    "* **Median**: The middle value in a sorted list of numbers. If the list has an even number of values, the median is the average of the two middle numbers. The median is less sensitive to outliers than the mean.\n",
    "\n",
    "* **Implausible Values**: Data points that are unlikely to be correct given the context of the dataset. For example, a person's age recorded as 250 years would be an implausible value.\n",
    "\n",
    "* **Extreme Values**: Data points that fall at the high or low end of the data distribution, but might still be plausible. For example, a person's height of 7 feet is extreme but plausible.\n",
    "\n",
    "* **Incorrect Formats**: Data that is not stored in the expected format, such as a date stored as a string (`'25-12-2023'`) when it should be a datetime object, or numerical data stored with currency symbols (`'$10.50'`).\n",
    "\n",
    "* **Duplicate Records**: Rows or entries in a dataset that are identical to other rows. Duplicate records can skew analysis and should often be removed.\n",
    "\n",
    "* **Outlier**: An observation point that is distant from other observations. Outliers can be caused by measurement errors or simply represent rare events. They can significantly affect statistical calculations like the mean.\n",
    "\n",
    "* **Encoding**: The process of converting categorical data (non-numerical) into a numerical format that can be understood by machine learning algorithms.\n",
    "\n",
    "* **Boolean Encoding**: A simple form of encoding where a categorical variable with two possible values is converted to a binary representation, typically `0` and `1`. For example, converting 'Yes' to 1 and 'No' to 0.\n",
    "\n",
    "* **Condensing the Categories**: The process of reducing the number of unique categories in a categorical variable. This is often done by grouping rare or similar categories into a single, broader category like \"Other.\"\n",
    "\n",
    "* **Reshaping a Dataset**: The process of changing the structure of a DataFrame, usually from a **wide** to a **long** format, or vice versa. This is commonly done using pandas functions like `pivot()` and `melt()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis3803",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
